[
["index.html", "Note", " Note This is PhD Thesis of Urszula Czerwinska. See the website urszulaczerwinska.github.io to learn more about me professional projects. "],
["resume.html", "Resumé", " Resumé Les tumeurs sont entourées d’un microenvironnement complexe comprenant des cellules tumorales, des fibroblastes et une diversité de cellules immunitaires. Avec le développement actuel des immunothérapies, la compréhension de la composition du microenvironnement tumoral est d’une importance critique pour effectuer un pronostic sur la progression tumorale et sa réponse au traitement. Cependant, nous manquons d’approches quantitatives fiables et validées pour caractériser le microenvironnement tumoral, facilitant ainsi le choix de la meilleure thérapie. Une partie de ce défi consiste à quantifier la composition cellulaire d’un échantillon tumoral (appelé problème de déconvolution dans ce contexte), en utilisant son profil omique de masse (le profil quantitatif global de certains types de molécules, tels que l’ARNm ou les marqueurs épigénétiques). La plupart des méthodes existantes utilisent des signatures prédéfinies de types cellulaires et ensuite extrapolent cette information à des nouveaux contextes. Cela peut introduire un biais dans la quantification de microenvironnement tumoral dans les situations où le contexte étudié est significativement différent de la référence. Sous certaines conditions, il est possible de séparer des mélanges de signaux complexes, en utilisant des méthodes de séparation de sources et de réduction des dimensions, sans définitions de sources préexistantes. Si une telle approche (déconvolution non supervisée) peut être appliquée à des profils omiques de masse de tumeurs, cela permettrait d’éviter les biais contextuels mentionnés précédemment et fournirait un aperçu des signatures cellulaires spécifiques au contexte. Dans ce travail, j’ai développé une nouvelle méthode appelée DeconICA (Déconvolution de données omiques de masse par l’analyse en composantes immunitaires), basée sur la méthodologie de séparation aveugle de source. DeconICA a pour but l’interprétation et la quantification des signaux biologiques, façonnant les profils omiques d’échantillons tumoraux ou de tissus normaux, en mettant l’accent sur les signaux liés au système immunitaire et la découverte de nouvelles signatures. Afin de rendre mon travail plus accessible, j’ai implémenté la méthode DeconICA en tant que librairie R. En appliquant ce logiciel aux jeux de données de référence, j’ai démontré qu’il est possible de quantifier les cellules immunitaires avec une précision comparable aux méthodes de pointe publiées, sans définir a priori des gènes spécifiques au type cellulaire. DeconICA peut fonctionner avec des techniques de factorisation matricielle telles que l’analyse indépendante des composants (ICA) ou la factorisation matricielle non négative (NMF). Enfin, j’ai appliqué DeconICA à un grand volume de données : plus de 100 jeux de données, contenant au total plus de 28 000 échantillons de 40 types de tumeurs, générés par différentes technologies et traités indépendamment. Cette analyse a démontré que les signaux immunitaires basés sur l’ICA sont reproductibles entre les différents jeux de données. D’autre part, nous avons montré que les trois principaux types de cellules immunitaires, à savoir les lymphocytes T, les lymphocytes B et les cellules myéloïdes, peuvent y être identifiés et quantifiés. Enfin, les métagènes dérivés de l’ICA, c’est-à-dire les valeurs de projection associées à une source, ont été utilisés comme des signatures spécifiques permettant d’étudier les caractéristiques des cellules immunitaires dans différents types de tumeurs. L’analyse a révélé une grande diversité de phénotypes cellulaires identifiés ainsi que la plasticité des cellules immunitaires, qu’elle soit dépendante ou indépendante du type de tumeur. Ces résultats pourraient être utilisés pour identifier des cibles médicamenteuses ou des biomarqueurs pour l’immunothérapie du cancer. "],
["summary.html", "Summary", " Summary Tumors are engulfed in a complex microenvironment (TME) including tumor cells, fibroblasts, and a diversity of immune cells. Currently, a new generation of cancer therapies based on modulation of the immune system response is in active clinical development with first promising results. Therefore, understanding the composition of TME in each tumor case is critically important to make a prognosis on the tumor progression and its response to treatment. However, we lack reliable and validated quantitative approaches to characterize the TME in order to facilitate the choice of the best existing therapy. One part of this challenge is to be able to quantify the cellular composition of a tumor sample (called deconvolution problem in this context), using its bulk omics profile (global quantitative profiling of certain types of molecules, such as mRNA or epigenetic markers). In recent years, there was a remarkable explosion in the number of methods approaching this problem in several different ways. Most of them use pre-defined molecular signatures of specific cell types and extrapolate this information to previously unseen contexts. This can bias the TME quantification in those situations where the context under study is significantly different from the reference. In theory, under certain assumptions, it is possible to separate complex signal mixtures, using classical and advanced methods of source separation and dimension reduction, without pre-existing source definitions. If such an approach (unsupervised deconvolution) is feasible to apply for bulk omic profiles of tumor samples, then this would make it possible to avoid the above mentioned contextual biases and provide insights into the context-specific signatures of cell types. In this work, I developed a new method called DeconICA (Deconvolution of bulk omics datasets through Immune Component Analysis), based on the blind source separation methodology. DeconICA has an aim to decipher and quantify the biological signals shaping omics profiles of tumor samples or normal tissues. A particular focus of my study was on the immune system-related signals and discovering new signatures of immune cell types. In order to make my work more accessible, I implemented the DeconICA method as an R package named “DeconICA”. By applying this software to the standard benchmark datasets, I demonstrated that DeconICA is able to quantify immune cells with accuracy comparable to published state-of-the-art methods but without a priori defining a cell type-specific signature genes. The implementation can work with existing deconvolution methods based on matrix factorization techniques such as Independent Component Analysis (ICA) or Non-Negative Matrix Factorization (NMF). Finally, I applied DeconICA to a big corpus of data containing more than 100 transcriptomic datasets composed of, in total, over 28000 samples of 40 tumor types generated by different technologies and processed independently. This analysis demonstrated that ICA-based immune signals are reproducible between datasets and three major immune cell types: T-cells, B-cells and Myeloid cells can be reliably identified and quantified. Additionally, I used the ICA-derived metagenes as context-specific signatures in order to study the characteristics of immune cells in different tumor types. The analysis revealed a large diversity and plasticity of immune cells dependent and independent on tumor type. Some conclusions of the study can be helpful in identification of new drug targets or biomarkers for immunotherapy of cancer. "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements I would like to thank my supervisors Andrei Zinovyev and Vassili Soumelis for guiding this project and enabling me to interact with their teams and sharing the resources. I would also like to thank the U900 lab and his head Emmanuel Barillot to generously equip me with the professional environment, the place and the tools. I address my gratitude to the TAC committee members Franck Pagès and Denis Thieffry for helping me organizing the jury and giving constructive comments along with my thesis, for being present, at least remotely despite severe weather conditions or travels. I would also like to express gratitude towards the jury for taking the time to assess this work. This is also the place to thank the AVIESAN foundation for funding my Ph.D. scholarship and the Pharmacology Faculty of Paris Diderot and me and specifically Chantal Guihenneuc for giving me the opportunity to teach in parallel of my Ph.D. I would also thank a lot Center of Interdisciplinary Research for equipping me with skills through numerous courses and Bettencourt Foundation for financing part of the training and sponsoring travel expenses. Special thanks to FdV coordinators: Sofie Leon, David Manset, Elodie Kaslikowski and Maria Molina Calvita for their availability and dynamism. Also, I would express my gratitude for supporting my application for French nationality to François Taddei, director of the FdV Ph.D. school. Thanks to all people I worked with in both teams: Arnau, Pauline, Gaelle, Paul, Cristobal, Luca, Laura, Laurence, Loredana, Jonas, Floriane, Maude, Philemon, Lilith, Paula, Michaly, Luis. To my FdV mates: Roberta, Juanma, Miza, Guillermo, and others. This work would never be possible without help and patience of my family, my partner Arnaud and his family. Especially, I would like to thank Arnaud, who managed to be with me on the daily basis, spent the endless hours correcting my writing and speaking, discussion about code and good practices, made available a computer server at home, painted furniture, offered me a roof and internet connection at low price, was making me laugh when I was coming home tired, angry or unmotivated, and just for being him adorable self. This thesis is dedicated to his father in whose case the available health-care solutions were not enough. I am proud to finish the thesis and face new professional adventures. I learned a lot about myself during this three years. Thank very much everyone whom I crossed on this path. I hope to meet you again one day. "],
["abbreviations.html", "Abbreviations", " Abbreviations TME DNA RNA (mRNA, miRNA) FACS scRNA-seq RNA-seq CAF TIL DGE BSS CRI ML AI TMA CNV CNA "],
["preamble-about-interdisciplinary-research.html", "Preamble about Interdisciplinary Research What does interdisciplinarity in science mean in XXI century? Strengths, Weaknesses Opportunities, Threats (SWOT) of an interdisciplinary PhD - personal perspective The origins of the PhD topic", " Preamble about Interdisciplinary Research We are not students of some subject matter, but students of problems. And problems may cut right across the borders of any subject matter or discipline. — Karl Popper The piece of work you are reading should harvest the fruit of an interdisciplinary research conceived in an interdisciplinary environment of Center for Interdisciplinary Research in Paris (CRI) in École doctorale Frontières du Vivant (FdV) and Institut Curie in groups Computational Systems Biology of Cancer and Integrative Biology of Human Dendritic Cells and T-cells. CRI’s main mission can be formulated as follows: to empower the students to take initiative and develop their own research projects at the crossroads of life, learning, and digital sciences. (“The CRI | Centre for Research and Interdisciplinarity” 2018) Interdisciplinarity has many definitions and meanings. According to the book Facilitating Interdisciplinary Research (Facilitating Interdisciplinary Research 2004) Interdisciplinary research and education are inspired by the drive to solve complex questions and problems, whether generated by scientific curiosity or by society, and lead researchers in different disciplines to meet at the interfaces and frontiers of those disciplines and even to cross frontiers to form new disciplines. For me, the essence of interdisciplinarity is the need to solve a complex problem, whatever expertise would be necessary to solve it. I consider that fighting cancer disease, deciphering cancer heterogeneity and interactions of immune system are causes worth an interdisciplinary effort. This is even truer in the era of big data, when the demand for quantitative tools is exponentially growing, in order to extract information and knowledge. Though this preamble I would like not only praise the interdisciplinary research but also underline possible limitations and constraints that come with it and which could affect this thesis. What does interdisciplinarity in science mean in XXI century? In the ancient history, being formed and practice multiple disciplines was not anything unusual which is strongly reflected in Greek philosophy initiating the dispute about the division and hierarchical classification of knowledge. (Slavicek 2012). Figures as Aristotle and Leonardo Da Vinci, that can be called homo universals served different disciplines from arts through history, natural sciences to mathematics. With time human knowledge about the word, i.e. natural sciences got bigger and bigger, to the point that it became hard to master all the disciplines. The specialization would allow to study in deep a certain subject and make possible discoveries about it. And even if, interdisciplinary efforts never stopped, for a long time they were not mainstream in scientific communities divided into academies, chairs and specialization. Different fields differ in term of concept, method, tools, processes and theories (Slavicek 2012). Thanks to division into scientific disciplines a sort of order is conserved across space and time. Hierarchical classification of knowledge comes from human nature. It can be observed that there is an increasing gap between disciplines along with specialization. advancing specialisation leads to gaps in the level of comprehension between individual disciplines and eventually gives rise to the demand for interdisciplinarity - in order to close the gaps between disciplines.(Slavicek 2012) It is not really clear why this gap must happen. Would it somehow reflect a human nature, the strong need to divide things into discrete categories rather than to see a continuum? Nowadays, the knowledge is accessible, we can profit from achievements of different disciplines thanks to easy means of communication. Two different terms can be defined to describe initiatives that use the knowledge of different specialties: multidisciplinarity which is a sum of efforts of different disciplines and interdisciplinarity that allows profiting from the synergy of multiple disciplines (Fig. .). With interdisciplinary research and education come flexibility, creativity and novelty but also limit of depth on ingested knowledge and possibilities of cross-interactions between disciplines. Figure .: Symbolic illustration of a sum (multidisciplinarity) versus synergy (interdisciplinarity), in an interdisciplinary project sum of thee disciplines A, B, C should have more value than a simple sum of disciplines: an interdisciplinary project should have an added value compared to a multidisciplinary one. Why are not all of the labs interdisciplinary? Scientists tend to resist interdisciplinary inquiries into their own territory. In many instances, such parochialism is founded on the fear that intrusion from other disciplines would compete unfairly for limited financial resources and thus diminish their own opportunity for research — Hannes Alfvén Crossing frontiers is not an easy task, and it was quite difficult in the beginnings of modern interdisciplinarity. Some examples of early interdisciplinary efforts of the 20th century are nicely described by Ledford et al. (Ledford 2015) in Nature special issue on Interdisciplinarity. It illustrates Theodore Brown in 1980s while trying to organize a new interdisciplinary research project and reorganise university space to engage exchange between students of different faculties, he encounters a lot of reluctance. And then there was the stigma. “Interdisciplinary research is for people who aren’t good enough to make it in their own field,” an illustrious physicist chided (Ledford 2015). The story seems to end up with a happy ending of 40-million US dollars grant and foundation of Beckman Institute for Advanced Science and Technology. However, recruiting an open-minded director to lead this unconventional organization was a struggle. Shortly, the structure became a model for others and met a great scientific and technological success. Even though, since then the idea of interdisciplinary research spread around the world. Yet, not all problems were overcome. “There’s a huge push to call your work interdisciplinary,” says David Wood, a bioengineer at the University of Minnesota in Minneapolis. “But there’s still resistance to doing actual interdisciplinary science”. First, the institutions, universities where research is performed should equip scientist with a passport to other disciplines, facilitate exchange, funding the interdisciplinary research, be accepting fusion of disciplines as new ones. Then, a proper communication between disciplines is necessary. Finally, developing an interdisciplinary research is extremely challenging as it often requires extra effort from an apprentice. Are all the disciplines independent units nowadays? Can we do molecular biology without technical, mathematical and computational support? Can we study cognitive science without knowledge of biology, physics and psychology? Can we advance medicine without basic research in biology, physiology, electronics? Bioinformatics and/or computational biology is an interesting case. Working in this field is being between biology, medicine, computer science, mathematics and statistics, the role of a computational biologist is sometimes reduced to a service. A biological lab may need a computational biologist to perform an analysis, restructure the data, that is needed for the biological discovery. Often, there is not enough space for research in computational biology itself, where the discovery does not depend on the original data but on tools and approaches to complex, data-intensive biological problems. It may happen also the other way round when a computational biologist asks a bench researcher to perform an experiment to prove his theoretical model. In both cases, the long-term interdisciplinary partnership would probably fail. Wet and dry researchers should collaborate as equal with important research advances on both sides to assure a long-term equilibrium. How did interdisciplinarity change over years? Are all disciplines affected equally? From the chart (Fig. .), we can notice that Social Studies of Medicine seems to be the most interdisciplinary field. In general Biology, Health and Biomedical Sciences seem to be more open into a flow of knowledge from other fields than humanities. On the extreme opposite of health, Clinical Medicine appears to be a very conservative field. Figure .: Interdisciplinarity of different fields. “From 1950-2014, a field’s position is determined by how much its papers cite outside disciplines (x-axis), and by how much outside disciplines subsequently cite its papers (y-axis). (Some years, certain fields have too few references to be plotted.)”. Reprinted by permission from Springer Nature (Van Noorden 2015) © 2015 Nature America, Inc. All rights reserved. Strengths, Weaknesses Opportunities, Threats (SWOT) of an interdisciplinary PhD - personal perspective I’m not good enough to do well something I dislike. In fact, I find it hard enough to do well something that I like — Jim Watson, Succeeding In Science: Some Rules Of Thumb (Csermely, Korlevic, and Sulyok 2007) Being formed first in a double major in biology and mathematics, then participating in interdisciplinary research projects during my master studies, I can witness that the learning curve of multiple disciplines can be steep. It is also often associated with the frustration of not going deep enough in all of the disciplines or the feeling of being overwhelmed by the amount of knowledge. Coming with the expertise in biology and mathematics, I got fascinated by complex biological systems. One way of study high-dimensional data is to reduce them into smaller interpretable units. This is what I tempted to achieve in this thesis in order to enrich our knowledge about tumor microenvironment and possibly contribute to orienting future research on immunotherapies. However, being an interdisciplinary researcher was not always a privilege. To which category do I belong? To whom should I present my work? I often asked myself these questions. I also often encountered lack of understanding where my methodological results were not bringing enough of biological insights. Or the constraints of my biological application seemed very obscured and complicated for mathematicians and my work often lacked important methodological advances. Does it mean that my work is not accurate, useless? Probably, for many, it is not enough. However, I still hope that our findings will be interesting to some. I enjoy working with data and statistics that serve an actual purpose. The Tab. . summarizes Strengths, Weaknesses, Opportunities and Threats (SWOT analysis) of an interdisciplinary project, in the way I perceive it. Table .: SWOT analysis of Interdisciplinary research. In SWOT analysis, Strengths, Weaknesses, Opportunities and Threats are enumerated. Strengths and Weaknesses are internal and Opportunities and Threats are external factors. Strengths (internal, positive) Weaknesses (internal, negative) Opportunities (external, positive) Threats (external, negative) Having a holistic view of the problem Not seeing details of the problem Mulitple possibilities to convey research Spending too much time filling knowledge gap Being supervised by multiple experts Following multiple, sometimes contradictory, advice on the same problem Take advantage of synergistic effect of fields Inhibiting effect of oppinions from different fields Joining expertises of different fields Not covering in details all the disciplines Doing a new discovery Obtaining too generic results Using new/non standard approach Experiencing steep learning curve Raising interest in different expert domains Not mastering the specific vocabulary of different fields Having better understanding of complex processes Being in constant need of help of domain experts Making progress Not being understood Higher creativity Creating a new field Being hard to classify/ fall into a category Having great flexibility Sovling many problems impossible to solve with traditional approach Being considered as superficial Feeling a thrill of adventure Being open Besides conducting research that crosses the boundaries of one discipline, I also could meet and work with inspiring people coping like me with filling the gap in understanding of an interdisciplinary work, multiple supervisors and report to many institutions. I gained (even if only superficial) understanding of many topics in mathematics, statistics, data science, immunology, cancer but also oral and written presentation skills, time and work management Is my thesis really interdisciplinary? Does biology profits from mathematics and mathematics from biology? I will let you judge it. What impact had biology on the statistical/mathematical modeling ? The practical problems, systems that go beyond theoretical formulations challenge the theoretical tools. In my work, I did my best to fuse theory and practice that should serve a biological application. I can image the project more complete if the results of my work would inspire changes in biological experiments, uncover new paths to follow for experimental biologists or translational researchers. The origins of the PhD topic The universe will lead me where I need to go. I am like a leaf in the stream of creation — Dirk Gently, Holistic detective When finishing my master I was looking for an interdisciplinary topic where I could deepen my quantitative skills and apply to a real-life healthcare problem. I came across a project proposed by Andrei Zinovyev in close collaboration with Vassili Soumelis. I was quite anxious that my knowledge of cancer immunology would not be sufficient to lead the project to a success. I recognize that the immune systems are is very complex and dynamic system and many years of expertise are needed to really grasp an understanding of it. I had a great chance to work hand in hand with domain experts that would suggest me the direction I should take in my research. The project started by causal exploration of different blind source separation or dimension reduction techniques and their ability to dissect bulk transcriptomic data into cell type-related units. We also faced an important problem of lack of gold standard validation data that would define efficiency and accuracy of different methods. I have spent void efforts working on a bulk transcriptomic data simulation framework, important statistical issues come our way and probably another few years of a different Ph.D. would be necessary to solve them. In the meantime, many tools dissecting tumor bulk transcriptome were published. Serving a similar purpose, they used different means and assumptions, which left a space for my project to continue. In my third year, I am finally publishing a tool that performs the analysis I developed together with the Sysbio team members, and I can apply it to a corpus of publicly available data to learn about the actual question: the immune system infiltrating cancers and the context-dependent signatures (see Chapters 4 &amp; 5). In a parallel project, I worked on an exploration of brand new data type: single cell transcriptomic (RNAseq) in the context of tumor microenvironment (see Chapter 6). We have also participated in Dream Idea Challenge, a project that aimed to put closer experimental and theoretical researchers (Annexe 1, (Azencott et al. 2017)) . I have collaborated in numerous projects within and outside my team. Some of the projects resulted in publications, such as my work on analyzing pDC subsets of X cancer Annexe2. Some others are in still preparation. I have attended X national and international conferences, where I presented posters, gave talks and I got awarded with distinctions for my work. Alongside with pursuing the compelling scientific research, I completed a wide variety of courses and I was teaching IT, Statistics and Mathematics at pharmacology faculty. Thanks to this extensive (&gt;300 hours) training over 3 years, I am equipped with soft skills that not only helped me to shape my thesis project on the go but also, I hope, will help me to succeed in my future career path. References "],
["organisation-of-the-dissertation.html", "Organisation of the dissertation", " Organisation of the dissertation As it is a fruit of an interdisciplinary work, I decided to introduce the topic from two perspectives: describe the biological and biomedical dimension of the topic (see Chapter 1), as well as, the mathematical dimension of the problem of separation of sources in complex mixtures (see Chapter 2). I hope, it will make the subject of my thesis easy to understand also for non-biologists or non-mathematicians. In the results part, I compare the reproducibility of blind source separation methods NMF and ICA (see Chapter 4), I cite our results on the estimating the Most Reproducible Transcriptomic Dimension (MSTD). I also apply ICA-based deconvolution to Breast cancer transcriptomes to prove its reproducibility Chapter 3. Then I introduce the DeconICA R package (see Chapter 5 ) and finally present results of an application of DeconICA and other tools to &gt;100 transcriptomic datasets (see Chapter 6). The second part of results is dedicated to my work on cell type heterogeneity (see Chapter 7). The manuscript finishes with Chapter 8 that contains discussion, conclusions and perspectives. In annexes, you can find publications to which I contributed during my doctorate that are not strictly linked with the topic of this thesis. INTRODUCTION Chapter 1: introduction to cancer biology and immunity, challenges in cancer immunotherapies and cancer immune phenotyping as well as data sources most commonly used to face the topic. Chapter 2: introduction to a problem of mixed sources in biological samples, an overview of blind source separation methods and supervised deconvolution methods, with focus on those applied to bulk transcriptome to uncover and quantify immune compartments RESULTS Chapter 3: Most Reproducible Transcriptome Dimension (MSTD) Chapter 4: application of ICA-based deconvolution to six breast transcriptomes Chapter 5: comparison of reproducibility of NMF and ICA methods Chapter 6: DeconICA R package Chapter 7: application of DeconICA R package and other tools to analyze &gt;100 transcriptome datasets of bulk cancer transcriptomes Chapter 8: study of immune cell types heterogeneity in tumor microenvironment using the innate immune map and scRNA-seq data DISCUSSION Chapter 9: discussion, conclussions and perspectives ANNEXES Other publications: pDC subsets Idea Dream Challenge "],
["intro.html", "Chapter 1 Immuno-biology of cancer 1.1 Cancer disease 1.2 Quantifying and qualifying immune infiltration (data) 1.3 From cancer phenotyping to immune therapies 1.4 Summary of the chapter", " Chapter 1 Immuno-biology of cancer This chapter will first introduce a short history of cancer with a focus on discoveries linking cancer and its environment. It will also describe the participation of TME in cancer development, progression and response to treatment. Most important types of data used to study cancer microenvironment will be discussed. I also introduce a link between tumor immune-biology and cancer phenotyping for development of immunotherapies. 1.1 Cancer disease According to GLOBOCAN study (Research in Cancer 2018), 14.1 million cancer cases were estimated to happen around the world in 2012. It touched 7.4 million men and 6.7 million women. It is estimated that the cancer cases will increase almost two-fold to 24 million by 2035. In France only, in 2012 there were 349426 cases of cancer, of which leading is Prostate cancer (16,3%) followed by Breast (14%) and Lung (11,5%). For a long time studying tumor was focused on tumor cells, their reprogramming, mutations. Cancer was seen as a disease of uncontrolled cells by the mainstream research. At the same time, the idea of the importance of the impact of other cells and structures on cancer cells was present but often not believed. A recent success of immunotherapies moved research focus to tumor cells in their context: tumor microenvironment. We will describe here what is the composition and role of the TME in tumor progression, diagnosis and response to treatment. 1.1.1 Historical understanding of cancer Cancer was historically described by a physician Hippocrates (460–370 B.C) (Sudhakar 2009). Even though there exist even earlier evidence of the disease. Hippocrates stated that the body contained 4 humors (body fluids): blood, phlegm, yellow bile and black bile. Any imbalance of these fluids will result in disease. Particularly the excess of black bile in an organ was meant to provoke cancer. For years, it was not known what factors cause cancer and it was easily confounded with other diseases. In the middle ages in the Renaissance Period, it was believed cancer is a punishment for the sins they committed against their god, that they deserved it to some extent. Until the 18th century, it was believed that cancer is contagious and is spread by parasites. In the 19th century, tumor cells started to be analyzed by pathologists. They were strike with their ability to proliferate uncontrollably, ability to spread and destroy the original tissue (NPR 2010b). Around the same time, leukocytes from the blood were first described by Gabriel Andra and William Addison. Just a few years later, in 1845 Bennett and Virchow described blood cells in leukemia (Fig. 1.1). Virchow is also a father of Chronic irritation theory (nowadays called chronic inflammation) that says that cancer is caused by local “irritation” and, incorrectly, that cancer cells spread like liquid resulting in metastasis. Figure 1.1: Illustration of Virchow’s cell theory. Virchow depicted different cells transformation due to irritation. (Virchow Rudolf 1847) In 1889, Stephen Paget introduced soil and seed hypothesis of metastases (Paget 1889). He formulates it as follows When a plant goes to seed, its seeds are carried in all directions, but they can only live and grow if they fall on congenial soil. Which is parallel to cancer cells disseminated by body fluids, and they can grow only tissues - “soil” that is predisposed to host the cancer cell - “the seed”. He focused on the importance of tissue characteristics that favorize tumor development as opposed to most researchers of his time that were focusing on the “seed” itself. In the 20th century, molecular causes started to be investigated. It was discovered that cancer could be caused by environmental factors, i.e. chemicals (carcinogens), radiation, viruses and also inherited from ancestors. Those factors would damage but contrary to a healthy condition they would not die. Also in 1909, Paul Ehrlich, called one of fathers of immunology and Nobel Prize laureate, indicated a link between immune system and tumor suppression (Ehrlich 1909). One of the remarkable first immunotherapy attempts can be attributed to William Coley, that practiced injecting streptococcus bacteria directly into patients after cancer surgery in 1891, later called “Colley vaccine”. However, the impact of this procedure on patients recovery was judged by scientific community as “unclear”. In 1968, Melvin Greenblatt and Philippe Shubik showed that tumor transplants secrete a substance stimulating the growth of blood vessels (Greenblatt and Shubi 1968), later identified as “tumor angiogenic factor (TAF)” by Judah Folkman in 1971 (Folkman et al. 1971). Folkman also suggested that TAF can be a target of a therapy itself. This was a revolutionary idea, at the time, as it did not target the tumor cells directly acted on their environment. During the 1970s, oncogenes and tumor suppressor genes were discovered. Oncogenes are genes that allow a cell to become a cancer cell, while the tumor suppressor genes would repair DNA or execute cell death of a damaged cell. A new dimension to cancer studies was added in the 1980s, epigenetic changes were proven to occur to both oncogenes and tumor suppressors (Feinberg and Vogelstein 1983; Greger et al. 1989), which are presently known as epigenetic markers used for diagnostics and therapeutic targets for cancer. In 1982, Aline van Pel and Thierry Boon (Van Pel and Boon 1982) discovered that a specific immunity to spontaneous tumor cells could be induced by vaccinating mice with mutagenized tumor cells. This raised an inspiration for many years of immune therapy development. In Napoleone Ferrara and colleagues identified the gene encoding vascular endothelial growth factor (VEGF) that was shown to stimulate the growth of endothelial cells proliferation in vitro and angiogenesis (blood vessels formation) in vivo (Leung et al. 1989). In 1999 for the first time, gene-expression was used to study cancer (leukemia) by Todd Golub, Donna Slonim and colleagues (Golub et al. 1999). Since the end of the 20th century, cancer screens are developed along with multiple strategies to fight the tumor. Most classical ones are based on the idea of removing tumor cells (surgery), killing tumor cells with DNA-blocking drugs (chemotherapy), radiation, inhibit cancer growth (hormonal therapy, adjuvant therapy and immunotherapy). As none of those methods is fully efficient, often a combination of treatments is proposed. Nowadays, science is aiming in the direction of targeted therapies and personalized treatment. The recent success of immunotherapies (discussed in Immunotherapies section attracted the attention the scientific community again to the context in which tumor cells are found. This context called Tumor Microenvironment, as well as the communication that happens within it between different agents nowadays studied differently with available knowledge of molecular biology, have become a popular scientific topic of the 21st century (Fig. 1.2). Figure 1.2: Percentage of publications containing the phrase “tumor immunotherapy” is growing, numbers retrieved on 17.01.2018 from Medline Trends (Corlan 2004) 1.1.2 Tumor Microenvironment as a complex system Tumor Microenvironment is a complex tissue that surrounds tumor cells. It is composed of different compartments (in solid tumors): Stroma: blood and lymphatics vessels, epithelial cells, mesenchymal stem cells, fibroblasts, adipocytes supported by extracellular matrix (EM) Immune cells: T cells, B cells, NK cells, Dendritic cells, Macrophages, Monocytes etc. Their proportion and specific roles vary significantly with tumor type and stage. Communication between the environmental cells and the tumor is critical for tumor development and has an impact on patient’s response to treatment. This communication between different compartments is bidirectional and all the players can influence each other. Depending on the nature and prevailing direction of those interactions different destiny is possible for each of the compartments, i.e. immune cells can be recruited to protect tumor cells or they can kill them directly. Many of the signals can be contradictory, many can suppress each other. Then is it possible to tilt this complex ecosystem into patients’ favor? Can we decipher the most important factors of this molecular knot and manipulate it? Next section describes different scenarios of interaction within TME in order to illustrate the complexity of TME and possible targets for cancer therapies. We recommend watching this video in order to visualize the TME and cancer thanks to 3D animations 1.1.2.1 Interactions between TME and Tumor Three scenarios can be considered to describe the relationship between TME and tumor cells: TME stimulates tumor growth and/or progression and/or impact negatively the response to treatment TME has no influence on tumor cells and disease development TME has a tumor-suppressive role and impact positively the response to treatment As it is presented in Section 1.1.1 these three hypotheses were gaining and losing popularity in the scientific and medical community over the decades. 1.1.2.1.1 TME as a foe: inflammation In 1863 Rudolf Virchow observed a link between chronic inflammation and tumorigenesis. According to Virchov theory, the genetic damage would be the “match that lights the fire” of cancer, and the inflammation or cytokines produced by immune cells should be the “fuel that feeds the flames” (Balkwill and Mantovani 2001). Therefore lymphocyte infiltration was confirmed by subsequent studies as a hallmark o cancer. The question one may ask is why our immune system is not enough to defend the organism from tumor cells as it does efficiently in a range of bacterial and viral infections? It is mainly because of the ability of tumor cells to inhibit immune response through activation of negative regulatory pathways (so-called immune checkpoints). Many examples can be cited on how TME facilitates tumor development (Fig. 1.3). For instance, in the early stages of tumorigenesis, some macrophage phenotypes support tumor growth and mobility through TGF-beta signaling. Also, it was shown that NK cells and myeloid-derived suppressor cells (MDSCs) have an ability to suppress immune defence i.e. immunosurveillance by dendritic cells (DCs), T cell activation and macrophage polarisation and they promote tumor vascularization as well. (Talmadge and Gabrilovich 2013; Gabrilovich, Ostrand-Rosenberg, and Bronte 2012) They create so-called niches that facilitate tumor colonization. T-regs and myeloid-derived suppressor cells can negatively impact natural immune defense and by these means allow growth and invasion of tumor cells (Taube et al. 2017). Another cell type, a part of ECM, fibroblast, or more precisely Cancer-Associated Fibroblasts (CAFs) have proven pro-tumor functions in breast cancer where they enhance metastasis (Dumont et al. 2013). The blood and lymphatic vessels maintain tumor growth providing necessary nutritive compound to malignant cells. Figure 1.3: The microenvironment supports metastatic dissemination and colonization at secondary sites. Different tumor sites can communicate through exosomes realized by tumor cells and also immune and stromal cells such as NK cells, CAFs and DCs. Reprinted by permission from Springer Nature (Quail and Joyce 2013) © 2013 Nature America, Inc. All rights reserved. According to (Hanahan and Coussens 2012) immune and stroma cells participate in almost all of Cancer Hallmarks (Hanahan and Weinberg 2000; Hanahan and Coussens 2012). Most of the hallmarks of cancer are enabled and sustained to varying degrees through contributions from repertoires of stromal cell types and distinctive subcell types. 1.1.2.1.2 TME seen as neutral In front of lack of definitive proof that TME can positively or negatively impact on tumor development, many scientists, in a long time, ignored the importance of this factor. Until the early-mid eighties, the TME research was mostly limited to angiogenesis and immune environment and most areas that are now driving the field were not represented. From the early 70s until the end of the 90s. the most accepted statement was that genetic alterations in oncogenes and tumor suppressor genes are both necessary and sufficient to initiate tumorigenesis and drive tumor progression. Therefore TME was not seen as an important element of the puzzle. The cancer geneticists, at the time, had a lot of influence on scientific community diminishing the work made on TME which were considered as “uninteresting” and definitely not “mainstream”. After the 90s, with the discovery of signaling molecules involved in the communication of TME like VEGF general opinion started to change. Furthermore, discoveries made by developmental biology field supported the hypothesis that microenvironment plays an important role in development which was later shown for tumorigenesis. Additionally, the success of immune vaccines starting with the tuberculosis vaccine Bacille Calmette-Guérin (BCG) in 1976 and finishing, at the moment with checkpoint inhibitors did not leave the scientific community indifferent. 1.1.2.1.3 TME as a friend: immunosurveillance As mentioned in Section 1.1.1 Paget proposed a hypothesis of “seed and soil” where the TME in a certain tissue (the soil) can either stimulate or suppress the metastasis (the seed). William Coley tested a possibility to trigger tumor-suppressive effect via stimulation of the immune system with bacteria. In the 1960s, the immune surveillance theory hypothesized “the ability to identify and destroy nascent tumors as a central asset of the immune system” (Sebeok 1976; Burnet 1970). Thus, the hypothesis that TME can have a positive role in tumor prognosis is not new. In modern immuno-oncology, the term immune-editing was introduced by Dunn et al. (2002) in 2002, to describe the relationship between the tumor cells and the immune system. The immunosurveillance through immune-editing can be summarized in three processes: elimination, equilibrium, and escape (Dunn et al. 2002). The elimination is the direct killing of cancer cells or growth inhibition by the immune system. The adoptive T cells and NK are actively involved in tumor killing and stimulate other immune cells. The CD8 + cytotoxic lymphocytes (CTLs) directly recognize tumor cells. Employing perforin- and granzyme-dependent mechanisms they can lyse tumor cells. The CD4 + T cells release factors to induce proliferation of B cells and to promote their differentiation to the antibody (Ab)-secreting plasma cells, activate macrophages. Macrophages use phagocytosis to eliminate cancer cells (Vesely et al. 2011). The tumor-infiltrating lymphocytes (TILs) have been associated with an overall good prognosis and better survival in different cancer studies. Moreover, abundance of CD3 + and CD8 + T cells, NK cells, and \\(\\gamma\\delta\\)T cells correlate with improved outcomes in epithelial ovarian cancers (Marquez-Medina, Salla-Fortuny, and Salud-Salvia 2012). Several studies report that the presence of the abundant immune infiltrates is correlated with a good prognosis or better survival (Kornstein, Brooks, and Elder 1983; Baxevanis et al. 1994; Naito et al. 1998; Pagès et al. 2005). Spontaneous regression of human tumors has been reported in cutaneous melanoma, retinoblastoma, osteosarcoma, etc. (Aris, Barrio, and Mordoh 2012). The equilibrium is the phase when cancer and immune cells coexist and their crosstalk is preventing metastasis. T cells are the main actor in maintaining the equilibrium. Progressively, the tumor cells become more immunogenic as they are not edited by the immune system (Bhatia and Kumar 2011). The state of tumor cells is then identified as “dormant” and active scientific reports investigate the possible molecular pathways that maintain dormancy or lead to escape (Teng et al. 2008). The immune escape is the final process when tumor cells impair the immune response. 1.1.2.2 Two-faced nature of immune cells: context-dependent functional plasticity A modern vision of TME-tumor interactions assumes that tumor can be directed to several molecular pathways. This direction is decided by signals that are native of tumor cell and/or coming from the microenvironment. Recent studies unveil ambivalent nature of immune cells in TME. While some as cytotoxic T cells, B cells and macrophages can manage to eliminate tumor cells. Treg cells role is to regulate expansion and activation of T and B cells. Depending on cancer type, they can be either pro- or anti-tumor. For example, as it has been shown for T-regs, that are usually associated with bad prognosis, they can be equally associated with improved survival (i.e. in colorectal cancer (Frey et al. 2010)). For innate immunity, there are widely accepted M1 (anti-tumor) and M2 (pro-tumor) extreme macrophages phenotypes in TME (Qian and Pollard 2010). Most of the statements seem to be context dependent and not valid universally across all cancer types. We already mentioned Macrophages phenotypic plasticity as well as the different behavior of EMC depending on tumor stage. From a more general point of view, it has been observed that immunodeficiency can correlate with high cancer incidence. Results of analysis based on observations of 25,914 female immunosuppressed organ transplant recipients, the tumor incidence was higher than predicted for multiple cancers. However, the number of breast cancer cases decreased which can be really disturbing if we need to decide on the role of immune defense in tumor progression (Stewart et al. 1995). This indicates that immune microenvironment can be cancer stimulating or inhibiting depending on the type of cancer and/or other factors. 1.1.2.3 Immune cell (sub)types in TME We are taught that a cell is the basic structural, functional, and biological unit of all known living organisms. A human body contains around \\(10^{14}\\) which is three orders of magnitude more than the number of stars in the Milky Way. This ensemble of cells is traditionally classified into cell types based on their phenotypical variety. for their immense number, the variety of cells is much smaller: only about 200 different cell types are represented in the collection of about \\(10^{14}\\) cells that make up our bodies. These cells have diverse capabilities and, superficially, have remarkably different shapes…. Boal (2002) In the description of TME, I have referred to cell types of immune cells as well-established entities of the immune system. However, the definition of cell types remains controversial and there is no consensus among researchers on how exactly a cell type should be defined. The notion of the cell-subtypes is even vaguer. The problem does not only concerns immune cells, most of the cell types of our organism, classified initially according to their morphology, seem to fulfill multiple functions. One can also relate cell-type problem to species problem where scientist also debates about where to draw the borders between species. This problem is widely generalized as “theory of types” (Slater 2013) in many disciplines as philosophy, linguistics, mathematics. In this chapter, I will limit the description to immune cell types. An immune cell can be described nowadays along many axes: Phenotype /surface markers Stability Morphology (expressed proteins) Ultrastructure (electron microscopy) Molecular data (gene expression, genotype, epigenome) Cell fate Cell of origin Function Depending on how well a cell is different from all other cells along with those axes, it will (or not) be defined as a distinct cell type. However, this comes with more or less subjective threshold on where the cells become significantly different. These thresholds can be established computationally or by an expert. The usual practice is a mix of both methods. Since the beginning of immunology, there was disagreement between pre-defined cell types and cell functions. Cette espèce de leucocytes a une grande ressemblance avec certains éléments fixes du tissu conjonctif, ainsi qu’avec des cellules endothéliales et des cellules de la pulpe splénique. On est donc souvent embarrassé, surtout lorsqu’on trouve ces leucocytes mononucléaires en dehors des vaisseaux, pour les distinguer des autres espèces de cellules mentionnées. — Elie Metchnikoff, Leçons sur la pathologie comparée de l’inflammation, 1891 The definition of cell types and subtypes is widely discussed today with the arrival of single cell technologies that allow a change of paradigm in cell classifications. Up to now, the top-down approach was mostly used. A pre-defined set of parameters describing a cell was fixed in order to select cells and then other parameters were measured. Now, it is possible to practice bottom-up approach where all (or some) parameters are measured for a single cell and then, depending on its distance from other cells, cell types are defined (Satija and Shalek 2014). The concept of “cell type” is poorly defined and incredibly useful — Allon Klein, Harvard Medical School Researchers recognize that the concept of cell type is artificial and a continuum of cell types is closer to the reality. According to Susanne Rafelski, A useful way to classify cells might thus be a multiscale and multi-parameter cell-type space that includes vectors for key intracellular organizational, dynamic, and functional features as well as tissue location, gene expression etc. Some, as Allon Klein, propose to introduce a concept of cell states which would better describe a cell depending on its context and function. However, an emerging challenge would be to connect cell states with historical cell types. (Ediorial Cell Systems 2017) . Another aspect of cells, that I am not approaching in this thesis, is time. Cells are shaped by their environment, intrinsic and extrinsic events and can change states, functions etc. Can one cell belong to different cell types depending on its trajectory? How to include the dynamic aspect of the cells into the classification? Thus, most scientists agree that used convention of cell types is not ideal and it is more matter of convenience than biological reality. This leaves a room to study cells and challenge existing classification. Describing cell types or cell states in the tumor microenvironment is extremely interesting as still little is know about the diversity of cell infiltrated in solid tissues. 1.1.2.4 Summary Cancer is a disease concerning milliards of people with a long history. Scientific community recognizes the role of the environment where the tumor cells find themselves as an important factor influencing tumor development, prognosis and response to treatment. TME is a complex environment that constantly interacts with tumor cells, where both tumor and TME influence and shape each other. Over the years, many interactions are being discovered and cell types re-defined and described in their context. However, lots of mechanisms and interactions of TME remains unknown due to very heterogeneous nature of this microenvironment. This leaves room for a more extensive investigation of TME. A therapeutic goal is target interactions that would be able to pivot the essential processes in tumorigenesis or tumor escape in order to put the cells “back on track” and facilitate anti-tumor therapies. These goals can be met thanks to the improvement of investigation methods, data quality and abundance. I will discuss the most important data types used in this project to investigate the TME. 1.2 Quantifying and qualifying immune infiltration (data) Nowadays, more and more biological data is produced. However, this proliferation of accessible resources is not proportional to generated insights and wisdom. In this thesis, I aim to generate Knowledge and Insights and we hope to generate some Wisdom (Fig. 1.4). In this section, we will introduce the foundation of our analysis: different data types that will be further discussed and explored in chapters that follow. Figure 1.4: From Data to Wisdom. Illustration of different steps that it takes to go from Data to generating Wisdom. It highlights that generating data is not equal to understanding it and additional efforts are needed to generate value. Image authored by Clifford Stoll and Gary Schubert published by Portland Press Limited on behalf of the Biochemical Society and the Royal Society of Biology and distributed under the Creative Commons Attribution License 4.0 (CC-BY) in (Ponting 2017). We will introduce most relevant data types that are used to study immune infiltration of tumors. 1.2.1 Cell sorting 1.2.1.1 Flow cytometry Flow cytometry is a laser-based technology. It uses marker genes: cell surface proteins to sort cells in different compartments. Nowadays, it permits quantification of the abundance of up to 17 cell surface proteins using fluorescently labeled antibodies (Papalexi and Satija 2017). However this techniques is not free from bias, our knowledge about cell markers is limited and several markers may not be relevant in some context. Moreover, the scientific community did not clearly agree on the marker choice even for popular and well-studied cell types which introduced additional heterogeneity when independent studies are compared. Also, the quality of antibodies may influence the results of the FACs analysis. Besides those limitations FACs remains quite a popular method for analyzing cells in complex tissues. It was among first methods that allowed molecular phenotyping of immune cells, a discovery of numerous subsets and their further functional interpretation. 1.2.1.2 Mass cytometry Mass cytometry (also known as CyTOF allows for the quantification of cellular protein levels by using isotopes. It allows to quantify up to 40 proteins per cell (Papalexi and Satija 2017). It also demands lower starting number of cells (1000 - 1000000), a realistic number that can be extracted from patient biopsy (Lyons et al. 2017). 1.2.2 Microscope Staining Using microscope technics, histopathological cuts are analyzed. The number of cells per a unit of area (i.e. mm\\(^2\\)) is defined either manually by a human or through diverse image analysis algorithms. Current pathology practice utilizes chromogenic immunohistochemistry (IHC) (NPR 2010a). Multiplexed approaches allow identifying multiple markers in the same histopathology cut. Modern techniques like imaging mass cytometry using FFPE tissue samples uses fluorescence and mass cytometry to identify and quantify marker proteins (Giesen et al. 2014). The main advantage of aforementioned technics the number of cells that can be analyzed and the information about the spatial distribution of the different cell types. The limiting factor, as for cell sorting methods, is the number of markers (~10-100) and consequently a number of cell types that can be identified (Schelker et al. 2017). The cell sorting methods and microscope staining are usually considered as a gold standard for multidimensional data techniques. The reason why they are not applied at large scale is the cost but also quite laborious and time-consuming sample preparation demanding a fresh sample. In contrast, the omics methods propose a more scalable way to measure tumor microenvironment. 1.2.2.1 Tissue Microarrays Tissue Microarrays aim to automatize “staining” techniques. A large number of small tissue segments can be organized in a single paraffin block where 100 tissue samples can be easily examined on one slide. A variety of molecular or microscopic method can be then applied to FFPE tissue including immunohistochemistry, FISH, and in situ hybridization (Wilczynski 2009). It is a technique in between traditional imaging and omic high-throughput. 1.2.3 omics In biological systems information is coded in the form of DNA that do not vary a lot between different individuals of the same species. To trigger a function in an organism, a part of the DNA is transcribed to RNA, depending on the intrinsic and extrinsic factors, and after additional modification messenger RNA (mRNA) is translated into a protein (i.e., digestive enzyme) that fulfill a role in the organism. The mRNA information (also called transcriptome) can be captured with experimental methods at high throughput (transcriptomics) and provides an approximation of the state of the studied system (i.e., a tissue). There is also information, not coded on the DNA sequence but in a pattern of chemical species that can regulate the state transition of DNA information. These additional regulators are called epigenome collectively and some of them, like methylation, can also be measured at high-throughput. 1.2.3.1 Transcriptome Transcriptomics measures the number of counts of mRNA molecules using high-throughput techniques. mRNA is the part of the genetic information that should be translated into proteins. It reflects the activity of ongoing processes in a cell. In contrast to DNA, mRNA concentration can be highly variable (Velculescu et al. 1997). This variability can be either “intrinsic” that reflect the stochastic process of cell machinery or “extrinsic” reflecting impact of factors upstream to mRNA synthesis (Satija and Shalek 2014). Transcriptome can be measured by microarrays or RNA-seq NGS technology. Microarrays remain cost-efficient and popular technique designed in 90. There exist two and one fluorescent color probes, both representing different challenges in experimental design for batch effect removal. RNA-seq, in contrast, uses sequenced RNA to quantify the expression. As not only selected genes (probes) are quantified, it can be used to study unknown parts of the genome. RNA-seq is also characterized by lower background noise than microarrays. Bulk transcriptome data are quite accessible nowadays. They can be obtained from either flash-frozen or formalin-fixed, paraffin-embedded (FFPE) tissue samples, including both surgically resected material and core needle biopsies (Schelker et al. 2017). The main flaw of transcriptomic data is that the reproducibility between different platforms is limited. As a result, direct comparison (direct merging, statistical difference tests) between two datasets produced by different platforms is not advised. There are 12 thousand genes that are matching between four sequencing platforms. Through gene names conversions much information is lost, and bias is introduced. Different strategies can be adapted to analyze bulk transcriptome. Cieślik and Chinnaiyan (2017) describes five groups of most popular approaches that can be applied to study transcriptome (Fig. 1.5). Despite a diversity of bioinformatic and statistical tools, the most popular differential approaches, mainly differential gene expression (DGE) based on the difference between two experimental conditions. Figure 1.5: Five categories of RNA-seq data analysis. Differential analyses: comparing two (or more) conditions, Relative analyses: comparing to an internal reference (average, base level), Compositional analyses: inferring cell types or groups of cell types (i.e., tumor purity), Global analyses: pan-tissue and pan-cancer analyses and Integrative analyses: compiling heterogeneous data types. Reprinted by permission from Springer Nature (Cieślik and Chinnaiyan 2017) © 2018 Macmillian Publishers Limited, part of Springer Nature. All rights reserved. RNA-seq data was proven to be a useful indicator for clinical applications (Mody et al. 2015; Oberg et al. 2016; Robinson et al. 2017). Its utility for immune profiling was demonstrated in many studies through the use of transcriptomic signatures to predict immunotherapy response or survival (Chen et al. 2016). In this work transcriptome data analysis falls into multiple categories: Compositional, Relative and aims to construct Global-level conclusions. 1.2.3.2 Single cell RNA-seq Described above methods of process DNA from hundreds of thousands of cells simultaneously and report averaged gene expression of all cells. In contrast, scRNA-seq technology allows getting results for each cell individually. This is tremendous step forward enhancement of our understanding of cell heterogeneity and opens new avenues of research questions. Continuous discovery of new immune subtypes has proven that cell surface markers that are used for phenotyping by techniques like FACS and immunohistochemistry cannot capture the full complexity. ScRNA-seq methods allow clustering known cell types in subpopulations based on their genetic features. ScRNA-seq is also able to capture particularly rare cell types as it requires much less of RNA material (1 ng isolated from 100-1000 cells) compared to ‘bulk’ RNA-seq ( ~ 1 μg of total mRNA transcripts ). It also allows studying cells at high resolution capturing the phenotypes in much more refined scale than previously (Papalexi and Satija 2017). This new data type also brings into the field new challenges related to data processing due to the volume, distribution, noise, and biases. Experts highlight as the most “batch effect”, “noise” and “dropout effect” (Perkel 2017). So far, there are no official standards that can be applied which makes data comparison and post-processing even more challenging. Up to date, there are around 70 reported tools and resources for single cell data processing (Davis 2016). A limited number of single-cell datasets of tumors are made publicly available, and more are to come. One can ask why then developing computational deconvolution of bulk transcriptome if we can learn relevant information from single-cell data. Firstly, that single cell data do not provide a straightforward answer to the estimation of cell proportions. The coverage is not full and sequenced single cells are not entirely representative of the actual population. For instance, neutrophils are not found in scRNA-seq data because of they are “difficult to isolate, highly labile ex vivo and therefore difficult to preserve with current single-cell methods” (Schelker et al. 2017). Besides, a number of patients included in published studies of range &lt;100 cannot be compared to thousand people cohorts sequenced with bulk transcriptome methods. This is mostly because single cell experiments are challenging to perform, especially in a clinical setting as fresh samples are needed (Schelker et al. 2017). Today, single cell technology brings very interesting “zoom in” perspective, but it would be incautious to make fundings from a restricted group of individuals universal to the whole population. Primary brake to the use of single cell technology more broadly might be as well the price that is nearly 10x higher for single cell sample compared to bulk (Core 2016). In this work, we are using single cell data in two ways. Firstly, in Chapter 5 we compare immune cell profiles defined by scRNA-seq, blood and blind deconvolution (problem introduced in Immune signatures section). Secondly, in Chapter 6 we use single call data of Metastatic melanoma generated by Tirosh et al. (2016) to demonstrate heterogeneity of subpopulations of Macrophages and NK cells. 1.2.3.3 Epigenome An epigenome can be defined as a record of the chemical changes to the DNA and histone proteins of an organism. Changes to the epigenome can provoke changes to the structure of chromatin and changes to the function of the genome (Bernstein, Meissner, and Lander 2007). Epigenome data usually contains information about methylation CpG island changes. In cancer, global genomic hypomethylation, CpG island promoter hypermethylation of tumor suppressor genes, an altered histone code for critical genes, a global loss of monoacetylated and trimethylated histone H4 were observed. Methylome profiles can also be used as a molecular signature of disease and potential diagnostic or predictive biomarker (Jeschke et al. 2017). 1.2.3.4 Copy number variation (CNV) and Copy number aberration (CNA) The differences between human genome come in the majority from Copy Number Variation (McCarroll and Altshuler 2007). CNV regions constitute 4.8–9.7% of the whole human genome (Zarrei et al. 2015). They can be reflected in structural variation that is duplication or deletion of DNA bases. CNV can affect a lot of base pairs of DNA code (deletion of more than 100 genes) and result in a phenotype change. In addition, there can be distinguished, Copy number alterations/aberrations (CNAs) that are changes in copy number that have arisen in somatic tissue (for example, just in a tumor), in contrast to CNV that originated from changes in copy number in germline cells (and are thus in all cells of the organism) (McCarroll and Altshuler 2007). CNV and CNA profiles can be associated with diseases or cancer subtypes. There exist disease-related exome panels that focus on regions with high copy variation, or the full exome can be sequenced using whole-exome sequencing (WES) (Yamamoto et al. 2016). 1.2.3.5 Spatial transcriptomics Spatial transcriptomics provides quantitative gene expression data and visualization of the distribution of mRNAs within tissue sections and enables novel types of bioinformatics analyses, valuable in research and diagnostics (Ståhl et al. 2016) It combines RNA-seq technology with spatial labeling which allows having a bulk gene expression of 10-20 cells with given space coordinates within the sample. It allows to localize regions of highest gene expression and perform Spatially Variable Genes (Svensson, Teichmann, and Stegle (2018)). Some attempts were already made to combine Spatial Transcriptomics and scRNA-seq (Moncada et al. 2018). It remains an early-stage technique, and so far it is not widely used, but it might be a future of omics to add spatial information as it can be essential for many research problems. 1.3 From cancer phenotyping to immune therapies This section outlines different methods of cancer immune phenotyping and progress in cancer therapies with a focus on immune therapies. It will link the ongoing research on TME with therapeutical potential. 1.3.1 Cancer immune phenotypes Since 20s century physicians decided on common nomenclature that classifies tumors into distinct groups that are relatively homogenous or that share common characteristic important for treatment and prognosis. Tumor typing should help to predict prognosis better, to adopt a therapy to the clinical situation, to enable therapeutic studies which are essential in proving any therapeutic progress. Most of the classifications are based on clinical data. Most common factors taken into account are the degree of local invasion, the degree of remote invasion, histological types of cancer with specific grading for each type of cancer, possibly various tumor markers, general status of the patient. However, cancers with similar morphological and histopathological features reveal very distinct patterns of progression and response to therapy (Galon et al. 2014). In the era of gene sequencing, gene and protein expression, as well as epigenome, can provide valuable complementary information. Therefore gene markers or proteomic abnormalities can be integrated into classification panel. One famous example is a gene signature PAM50 (Parker et al. 2009) used for prediction of patients’ prognosis in breast cancer, patented as a tumor profiling test. Since the increase of importance of the immunotherapies, researches proposed several ways to classify tumors based on their microenvironment. Given different parameters describing TME, cancers can be sorted into groups that show similar characteristics. We will discuss most common frameworks that allow for phenotype cancers based on the TME. The localization of the immune cells can be an indicator of the state and response to the therapy (Bindea et al. 2013). The most standard approach is to convey an analysis of histopathological cuts to asses the number of infiltrating lymphocytes (TILs). Two typical patterns are usually identified: “hot” - immune inflamed and “cold” - no active immune response (Berghoff, Kather, and Jäger 2018). Chen and Mellman (2017) describe classification into inflamed and non-inflamed tumors, where non-inflamed phenotypes: can be further split into the immune-desert phenotype and the immune-excluded phenotype (Fig. 1.6). The inflamed phenotype is characterized by the abundant presence of immune cells: T cells, myeloid cells, monocytes in tumor margin. Along with the immune cells, due to their communication, a high expression of cytokines is characteristic for this phenotype. According to Chen and Mellman (2017), this is a mark that an anti-tumor response was arrested by the tumor. The inflamed phenotype has shown to be most responsive to immunotherapies. In the immune-excluded phenotype, the immune cells are present as well but located in the stroma (Herbst et al. 2014), sometimes penetrating inside the tumor. However, when exposed to checkpoint immunotherapy, T cells do not gain the ability to infiltrate the tumor; therefore the treatment is inefficient. The immune-desert main features are little or no presence of immune cells, especially T cells. Surprisingly, these tumors have been proven to respond rarely to the checkpoint therapy (Herbst et al. 2014). In non-inflamed tumors, cytokines associated with immune suppression or tolerance are expressed. Figure 1.6: Cancer-immune phenotypes: the immune-desert phenotype (brown), the immune-excluded phenotype (blue) and the inflamed phenotype (red). The immune-desert phenotype is characterized by a paucity of immune cells and cytokines. In the immune-excluded phenotypes, the T cells are often present but trapped in the stroma, enabled to migrate to the tumor site. The immune-inflamed phenotype is rich in immune cells and the most responsive to the immune checkpoint therapies. Reprinted by permission from Springer Nature (Chen and Mellman 2017) © 2017 Macmillian Publishers Limited, part of Springer Nature. All rights reserved. A presence of immune phenotypes was confirmed by for example by Becht et al. (2016) in colorectal cancer, where after deconvolution of bulk tumor profiles, patterns of immune and stromal cells abundance was matching four cancer subtypes. The good prognosis was related to cytotoxic response and bad prognosis to lymphocytes and cells of monocytic origin. According to Gajewski et al. (2006), the immunogenicity of the tumors can be explained by tumor-intrinsic factors and tumor-extrinsic factors. Tumor-intrinsic factors are the neoantigen load and frequency, the mutational load, the expression of immunoinhibitors and immunostimulators (e.i. PD-L1), and alteration of HLA class I molecules. Tumor-extrinsic factors include chemokines regulating T cell trafficking, infiltration of effector TILs and immunosuppressive TILs, and soluble immunomodulatory factors (cytokines). 1.3.2 Scoring the immune infiltration Experimental techniques and computational tools enabled us to characterize and classify TME with multi-omics data. Here I present a short list of most influencing and complete analysis aiming to redefine tumor phenotypes based on the immune infiltration, with a focus on computational techniques. 1.3.2.1 Immunoscore Jerôme Galon lab in Paris authors one of the most recognized scoring method, based on fluorescent images and names Immunoscore. The Immunoscore ranges from 0 to 4 and it is based on the density of lymphocyte populations CD3/CD45RO, CD3/CD8, or CD8/ CD45RO. It also takes into account the spacial position of the cells: the tumor core and margins (Galon et al. 2012). It was successfully applied to colorectal cancer to predict patients’ survival (Anitei et al. 2014). Since then, it resulted in numerous application to many cancer types. Immunoscore has been recently validated in a large cohort international independent study (14 centers in 13 countries) as a relevant prognostic score of time to recurrence, defined as the time from surgery to disease recurrence (Pagès et al. 2018). The immunoscore is an interesting indicator, especially in the scope of clinical applications, although it does not tell us a lot about underlying biology. It is also limited to a few cell types while it may be that in some cancer types or patients, the system requires more detailed or rich analysis of a larger panel of cells. 1.3.2.2 Spatiotemporal dynamics of Intratumoral Immune Cells of Colorectal Cancer Bindea et al. (2013) published a quite complete, and supported with strong experimental evidence, immune landscape of colorectal cancer. Authors introduced the immunome compendium containing 577 cell-type-specific genes, derived from analysis of a significant corpus of publicly available data. They used it to analyze CRC large transcriptomic data (105 patients). Using qPCR (more sensitive technique than microarray) expression of 81 “representative” genes from the compendium was investigated in 153 CRC patients. This study validated correlation of markers of the same type and also revealed the correlation of different cell-type markers (i.e., T-cells and NK or Th and macrophages). The data matrix was grouped into 3 clusters which were corresponding to 1) tumor 2) adaptive 3) innate immune responses. Besides, spatial positioning of markers was visualized thanks to Tissue Microarray technology in samples from 107 CRC patients distinguishing marker densities in tumor center and tumor margin areas. This was followed by an in-depth study of chemokines expression and genomic alterations. Also, authors validated potential prognostic biomarkers in murine orthotopic CRC models. In summary, using marker genes measured and visualized with different data types of CRC, a high inter-patient heterogeneity was observed. It was confirmed that the immune landscape evolves over time (tumor stages). Adaptive immunity cells were associated with the core of the tumor and the innate ones with the tumor margin. A mechanism involving CXCL13, Tfh cells, B cells and IL-21 was identified as associated with good prognosis. 1.3.2.3 Immunophenoscore Different approaches, sub-typing oriented, are based principally on gene expression patterns. Most commonly, machine learning supervised algorithms are trained to match known phenotype (established with microscopy or with clinical features) to genetic patterns, or an unsupervised clustering is used to discover new classification. An example of well-formulated classification framework is Immunophenoscore (Charoentong et al. 2017), based on the publication of Angelova et al. (2015), where methylome, transcriptome and mutation of TCGA CRC dataset (n = 598) was used to describe immunophenotypes. Later on, it was reduced to gene expression indicator and summarised in the form of a score. This scoring scheme is based on the data of 20 solid tumors, using the expression of marker genes selected by a machine learning algorithm (random forest) for best prediction in each cancer. These indicators can be grouped into four categories: MHC molecules (MHC) Immunomodulators (CP) Effector cells (EC) Suppressor cells (SC) The immunophenscore (IPS) is calculated on a 0-10 scale based on the expression of genes in each category. Stimulatory factors (cell types) impact the score positively and inhibitory factors (cell types) negatively. Z-scores \\(\\geq\\) three were designated as IPS10 and z-scores \\(\\leq\\) 0 are designated as IPS0. A similar conceptual framework called cancer immunogram was proposed by Blank et al. (2016) included seven parameters: tumor foreignness (Mutational load), general immune status (Lymphocyte count), immune cell infiltration (Intratumoral T cells), absence of checkpoints (PD-L1), absence of soluble inhibitors (IL–6, CRP), absence of inhibitory tumor metabolism (LDH, glucose utilisation), tumor sensitivity to immune effectors (MHC expression, IFN-γ sensitivity). Charoentong et al. (2017) claim that the immunophenoscore can predict response to CTLA-4 and anti-PD-1. Nonetheless, the details of the use of cancer immunogram in practice remain unclear and the result could be sensitive to patients’ and data heterogeneity as no standardization was proposed. It should also be validated in a systematic, independent study. 1.3.2.4 The immune landscape of cancer Table 1.1: Six immunological subtypes of cancer. The general characteristic of subtypes generated by Thorsson et al. (2018) as described in the original publication. Cluster Features Macrophage..lymphocyte Th1.Th2 Proliferation Intratumoral.heterogeneity Other C1 Wound healing Balanced Low High High C2 IFN-γ dominant Lowest Lowest High Highest Highest M1 and CD8 T cells C3 Inflammatory Balanced High Low Lowest Highest Th17 C4 Lymphocyte depleted High Minimal Th Moderate Moderate C5 Immunologically quiet Highest Minimal Th Low Low Highest M2 C6 TGF-β dominant High Balanced Moderate Moderate Highest TGF-β signature Thorsson et al. (2018) performed a multi-omic analysis of TCGA datasets that allowed them to define six subtypes that are valid across cancer types (see Tab. 1.1 ). Authors selected eight indicators to define these six phenotypes: differences in macrophage or lymphocyte signatures Th1:Th2 cell ratio extent of intratumoral heterogeneity aneuploidy extent of neoantigen load overall cell proliferation expression of immunomodulatory genes prognosis These indicators were selected among many other indicators though machine learning (elastic net regression) for the best predictive power of survival. All the data and computed parameters can be accessed at CRI iAtlas Portal. Among the six phenotypes C3 (Inflammatory) has the best-associated prognosis while C1 (wound healing) and C2 (IFN-\\(\\gamma\\) dominant), much less favorable outcome. This again illustrates the ambivalent nature of the immune system as the best, and the worst prognosis is associated with immunologically active tumors. C4 (lymphocyte depleted) and C6 (TGF-\\(\\beta\\) dominant) subtypes had the worst prognosis. The content of immune cells was determined using different tools and data types (expression, DNA methylation, images, etc.) We can learn a lot from the study. However, it seems difficult to integrate the methods into an ordinary practice because different data levels are necessary for the same samples to compute all the indicators. 1.3.2.5 A pan-cancer landscape of immune-cancer interactions in solid tumors A different classification was proposed by Tamborero et al. (2018), also using TCGA data. They distinguished 17 immune infiltration patterns based on the immune cell proportions and 6 different clusters based on cytotoxicity measure across all cancer types (named immune-phenotypes) that were finally summarized in three groups: cytotoxic immune infiltrate, infiltrate with more immune-suppressive component and poor immune infiltrate. According to the analysis, one of the most critical factors is cytotoxicity. Tumors with high cytotoxicity were characterized by low clonal heterogeneity, with gene alterations regulating epigenetic, antigen presentation and cell-cell communication. The medium-level cytotoxic tumors had activated invasion and remodeling of adjacent tissue, probably favorable to immune-suppressive cells. The low cytotoxicity subgroup of tumors had altered: cell-cycle, hedgehog, \\(\\beta\\)- catenin and TGF-\\(\\beta\\) pathways. This result roughly overlaps with the one of Thorsson et al. (2018). The survival analysis based on the six immune-phenotypes revealed that for most cancer types, high cytotoxic tumors are associated with better survival. To evaluate tumor environment cells, authors used gene set variation analysis (Hänzelmann, Castelo, and Guinney 2013) with a set of pre-defined cell-type markers. Another important conclusion of Tamborero et al. (2018) is that tissue of origin is not the only important factor shaping cell-type patterns in tumors. However, the least infiltrated tumors were lung, uterine and bladder cancers, while the most infiltrated were pancreatic, kidney, skin cancers and glioblastoma. They also analyzed cancer cell pathways after computational purification of tumor samples (subtraction of the immune signal) to better understand cancer signaling. A different approach is to characterize tumors based on signaling pathways organized in functional modules. 1.3.2.6 Immune maps Another way to summarize tumor phenotype can be through the use of molecular maps. Atlas of Cancer Signaling Network (ACSN) (Kuperstein et al. 2013; Kuperstein et al. 2015) is a pathway database that contains a collection of interconnected cancer-related signaling network maps. An additional feature is ACSN web-based Google-maps-like visualization of the database. User data can be projected on the molecular map (for example gene/protein expression from user data can be paired with entities on the map ). ACSN 2.0 contains Cancer cell map and TME map (at the time: angiogenesis, innate immune map, T-cell signaling maps). All separate maps are available in Navicell website. Through projection of the data on the innate immunity map, one can see if a tumor sample is characterized by pro- or anti-tumor activated pathways due to the organization of the map layout. Also, different CAF subtypes were characterized by the CAF specific map in (Costa et al. 2018). Kondratova and colleagues (including myself) used the innate immune map to characterize NK and Macrophages subtypes (see Chapter Z). 1.3.2.7 Summary Despite all scientific efforts, the gene expression-based classifications are not yet used in clinics. The measured multi-panel mRNA expression, which can be included into the category of In Vitro Diagnostic Multivariate Index Assay (IVDMIA) (Győrffy et al. 2015; Ross et al. 2008), may be a future of TME-based cancer classification, diagnosis and treatment recommendation (Gnjatic et al. 2017). For this best tools need to be used to evaluate the state of TME and tumor-stroma-immune cells communication properly. 1.3.3 Immune signatures - biological perspective A gene signature is a single or combined group of genes in with a uniquely characteristic pattern of gene expression that occurs as a result of an altered or unaltered biological process or pathogenic medical condition (Itadani, Mizuarai, and Kotani 2008; Liu et al. 2008). They can be classified based on their form: metagene gene list weighted gene list A term metagene or eigen gene describes an aggregated pattern of gene expression. The aggregation can correspond to simple mean of samples or can be obtained though matrix factorisation or source separation techniques, clustering. A metagene usually provides values for all measured genes (all probes) in contrast to a wighted gene list where weights are associated with selected genes. Gene lists are simple enumeration of transcripts names or gene identifiers. Application of gene list is often limited to gene enrichment analysis tools or gene selection from the data. An alternative is a weighted gene list or ranked gene list, where genes are ranked according to their importance. Often the ranks is obtained though comparison between two conditions or test/control. They can be also based on absolute gene expression values(Lyons et al. 2017). One possible problem with this weighted gene list can be platform dependence. There exist a big choice of databases storing collections of signatures. They contain gene expression and other genomic data such as genotype, DNA methylation, and protein expression data attributed to some condition of reference. A big collection of immune signatures are regrouped by Immunological Genome Project (IGP, ImmGen) (Heng et al. 2008). Gene expression of protein coding genes measure in mice immune cells, ex vivo, in different conditions (drug treatment, perturbations) were regrouped in this ressource. A different ressource Immuno-navigator (Vandenbon et al. 2016) that stores information about human and murine immune genes and co-expression networks. ImmuneSigDB is a collection of gene-sets that describe immunity and inflammation in transcriptomic data (Godec et al. 2016) and a part of popular MSigDB ressource used commonly for gene set enrichment analysis (GSEA) (Subramanian et al. 2005). They can also be classified based on their use: prognostic signatures predictive signatures diagnostic signatures specific signatures The prognostic signatures can distinguish between patients with a good or from patients with bad prognosis when deciding to assign a patient to a therapy. The predictive signatures are able to predict treatment benefit between experimental and/or nontraditional treatment groups vs. control, i.e. in clinical trials (Michiels, Ternès, and Rotolo 2016). The diagnostic signature, also called biomarkers can be used for detection of a disease in a patient, like for example in blood tests. The specific signatures should describe with robustness and reproducibility the same group of cells, or patients, or condition with respect to other considered groups. For instance, in the context of cell-types, among studied cell-types a specific signature will distinguish only one cell type. In the context of cancer subtypes, it will indicate clearly one subtype among others. Examples of predictive and prognostic gene signatures, used in clinical practice are Oncotype DX, EndoPredict, PAM50, and Breast Cancer Index for breast cancer (Harris et al. 2016). Studies discussed in this Chapter showed plausible importance of immune-related signals in cancer therapy. However, there is no no immune-related gene signatures used in clinical practice currently. This can be because of the lack of consistency of genes, both within the same tumor type and among different tumors that can be found in the signatures (Chifman et al. 2016). Difference in gene expression of different cell populations were found even intra- and interlabs. This difference can be due to confounding factors like stress or to contamination (Heng et al. 2008). In many studies specifc signatures of cell types are used. They seem to be good in discriminating between broad lineages of cell type, such as lymphoid and myeloid. Although thier capacity to describe cell states and cell subtypes is more discutable (Chifman et al. 2016). Another matter is that cell type signatures are often obtained in model organisms or extracted from different tissue (i.e. blood-derived signatures vs cancer-derived signatures). the gene expression profiles of tumour-associated immune cells differ considerably from those of blood derived immune cells (Schelker et al. 2017) With emergence of single-cell signatures, there are new horizons of gene signatures to be discovered. Especially signatures of rare cell types in solid tissues. Yet, it is up to researches to cross validate single cell signatures with different types of data as scRNA-seq is not free of platform and post-processing bias. Immune signatures will be also discussed as a part of deconvolution pipeline in the Chapter 2 under the section about basis matrix in mathematical terms. 1.3.4 Cancer therapies Cancer is a complex disease. Up to date, no uniform and fully effective treatment were proposed, and usually different strategies are tested to kill tumor cells. Surgery is one of the oldest methods. The cancer is removed from the patient body. There are different ways, more or less invasive, that it can be performed. It is usually applied for solid tumor contained in a small area. Radiation Therapy uses high doses of radiation to eliminate tumor cells and shrink tumor mass. It can be applied externally or internally. Chemotherapy uses a drug (or a combination of drugs) that kill cancer cells, usually altering cell proliferation and growth. The drawback of radiotherapy and chemotherapy are substantial side effects. Hormone therapy modulate hormone levels in the body in order to inhibit tumor growth in breast and prostate cancers. In leukemia and lymphoma, can be applied stem cell transplants that restore blood-forming stem cells destroyed by the very high doses of chemotherapy or radiation therapy that are used to treat certain cancers. Alternatively, targeted therapies represent a more focused strategy that aims to be more efficient and cause fewer side effects than systematic therapies. Two main types of targeted therapies are small-molecule drugs and monoclonal antibodies. Targeted therapies usually aim to stimulate/inhibit a selected molecular function. Particular types of targeted therapies are Immunotherapies. Through activation/inhibition of immune regulatory pathways, it stimulates the immune system to destroy malignant cells. A continuation of targeted therapies is precision medicine approach. It is based on genetic information to specify patient’s profile and find a suitable treatment. A number of innovative treatments targeting a specific change in tumor ecosystem are being tested presently in precision medicine clinical trials (Institute 2017). 1.3.5 Recent progress in immuno-therapies The immunotherapies, in contrast with other types of cancers therapies discussed in the previous section, aim to trigger or restart the immune system to defend the organism and attack the malignant cells without provoking persisting inflammation state (Predina et al. 2013) The idea of stimulating the immune system to fight malignant cell was not born recently. For a long time, a possibility of development of an anti-cancer vaccine has been investigated. Unfortunately, this idea faced two essential limitations 1) lack of knowledge of antigens that should be used in a vaccine to stimulate cytotoxic T cells successfully) the ability of cancer to block the immune response also called immunostat. Despite those impediments works on anti-tumor vaccines do not cease (Palucka and Banchereau 2013). A very recent promising an in-situ anti-tumor vaccine was proposed by Sagiv-Barfi et al. (Sagiv-Barfi et al. 2018). The therapy tested in mice would be based on local injections of the combination of “unmethylated CG-enriched oligodeoxynucleotide (Cp-G) - a Toll-like receptor 9 (TLR9) ligand and anti-OX40 antibody. Low doses of CpG injected into a tumor induce the expression of OX40 on CD4+ T cells in the microenvironment in mouse or human tumors. An agonistic anti-OX40 antibody can then trigger a T cell immune response, which is specific to the antigens of the injected tumor”. Sagiv-Barfi et al. claim this therapy could be applied to all tumor types, as long as they are leucocyte-infiltrated. As a local therapy, in situ vaccination should have fewer side-effects than systematic administration. It is now undergoing clinical trials to test its efficiency in human patients. Another idea involving using the immune system as a weapon to fight cancer would be the use of genetically modified patient’s T-cells, carrying CARs (chimeric antigen receptors) (Jackson, Rafiq, and Brentjens 2016). After an extended period of small unsuccessful trials, recently in 2017, two CAR T-cell therapies were accepted, one to “treat adults with certain type of large B-cell lymphoma” (Health and Services 2017b), other to treat “children with acute lymphoblastic leukemia (ALL)” (Health and Services 2017a), which are, at the same time, the first two gene therapies accepted by FDA. However, the two most promising immuno-related strategies with proven clinical efficiency are based on blocking so-called immune checkpoint inhibitors: cytotoxic T-lymphocyte protein 4 (CTLA4) and programmed cell death protein 1 (PD-1). The anti-CLTA4 antibodies blocks repressive action of CLTA4 on T-cells and they become therefore activated. It was shown efficient in melanoma patients and accepted by FDA in 2015 as adjuvant therapy for stage III metastatic melanoma patients (Health and Services 2015). PD-1 is a cell surface receptor of T cells, that binds to PD-L1/PD-L2. After binding, an immunosuppressive pathway is activated and T cells activity is dampened. An action of an anti-PD-L1 antibody is to prevent this immune exhaustion (Chen and Mellman 2017). A stepping stone for anti-PD-L1 therapies was approval of Tecentriq (atezolizumab) for Bladder cancer (Health and Services 2016a) and anti-PD1 Keytruda (pembrolizumab) initially accepted for NSCLC and further extended to head and neck cancer, Hodgkin’s lymphoma, gastric cancer and microsatellite instability-high cancer (Health and Services 2016b). Since that breakthrough, other anti-PD-L1 or anti-PD1 antibodies were accepted or entered advanced stages of clinical trials (Wolchok 2015). A short history of immunotherapy FDA-accepted treatments can be found in Fig. 1.7 Figure 1.7: This timeline describes short history of FDA approval of checkpoint blocking immunotherapies up to 2017. Reprinted by permission from Springer Nature (Taube et al. 2017) Macmillan Publishers Limited, part of Springer Nature. All Rights Reserved. The main drawback of immunotherapies is heterogeneity of response rate, which can vary, i.e., from 10–40% in case of PD-L1blocking (Zou, Wolchok, and Chen 2016), suggesting that some patients can have more chances than others to respond to immune therapy. So far, it has been shown that anti PD-L1 therapies work more effectively in T cell infiltrated tumors with the exclusion of Tregs because of lack of difference in expression of FOXP3 in responding and the non-responding group of patients (Herbst et al. 2014). Also, some light has been shade by Rizvi et al. (2015) who connected mutational rate of cancer cells to the chances of response to immunotherapy. Despite those findings, the precise qualifications of patients that should be sensitive to immunotherapy are not defined (Pitt et al. 2016). As most patients do not answer to immunotherapies, it stimulates researches to look for better biomarkers and patient stratifications, and pharmaceutical industries to discover new immune checkpoints based therapies. 1.4 Summary of the chapter Cancer remains a critical health problem of our era that touches many people. Tumor cells are interacting with their microenvironment (called Tumor Microenvironment (TME)) including normal cell, stromal cells and a variety of immune cells. These cells can have a role in disease progression and response to treatment. A modern approach to modulate TME was proposed through an application of immune therapies. Many researchers aim to link the composition and state of TME with patients clinical features and survival. It has been shown that in some cases certain cell types are beneficial to tumor development and some are not. However, a case by case approach of personalized medicine may be necessary to understand the inter-patient heterogeneity fully. A new way to classify cancers based on their TME is called immunophenotyping. There are no well-established procedures to perform it yet, but it can be integrated shortly into the clinical classification of tumors. It is a subject of active research to find new biomarkers and signatures of different factors governing the TME. Much interest is directed nowadays towards immune cells. Given that traditional cell-type definitions can be questioned in the cancer context, new cell states and functional subtypes are being redefined by researchers. There are many experimental techniques, like immune staining and FACS, which allow an in-depth study of the immune system. They require vital preparation steps and fresh samples. Also, a limited number of variables can be observed through this techniques, and knowledge-based hypotheses are necessary. On the other hand, high-throughput omic data allow measuring of the all system at the same time as they can measure all referenced units (genes/methylation sites/ copy number aberrations) from FFPE samples that can be stocked for a long time. Discovery-Type studies are then, and new biomarkers can be discovered. Particularly suited for studying complex biological systems is scRNA-seq as it provides gene expression profile of each cell without a compulsory use of marker genes, indispensable in other techniques to define cell-types. However, this technique remains quite costly, and it is not yet optimized. Therefore to have a very detailed system-level view of the TME with traditional experimental techniques an uncountable amount of work and resources would be necessary. Using omic techniques system approach is possible, however, to embrace fully the data complexity computational tools are needed. From data generation to analyze different statistical and mathematical challenges need to be faced before arriving at valid biological results and interpretations. As I will present in the next chapter, in order to solve the problem of extraction of cell-type heterogeneity from cancer bulk omic data, a number of approaches were developed. References "],
["methods.html", "Chapter 2 Mathematical foundation of cell-type deconvolution of biological data 2.1 Introduction to supervised and unsupervised learning 2.2 Types of deconvolution 2.3 Cell-type deconvolution of bulk transcriptomes 2.4 Deconvolution of other data types 2.5 Summary of the chapter", " Chapter 2 Mathematical foundation of cell-type deconvolution of biological data In the previous chapter, I presented state-of-art of the current immuno-oncology research that has to embrace the vast complexity of cancer disease and the immune system. One part of this complexity can be explained by the presence and quantities of tumor-infiltrating immune cells, their interactions with each other and the tumor. In this chapter, I will discuss how mathematical models can be used to extract information about different cell-types from ‘bulk’ omics data or how to de-mix mixed sources composing the bulk samples. To start with, I will introduce you to basic concepts of machine learning. Then I will focus on approaches adapted for cell-type deconvolution. In a literature overview, I will depict the evolution of the field as well as discuss the particularities of different tools for estimating presence and proportion of immune cells within cancer bulk omic data. 2.1 Introduction to supervised and unsupervised learning Machine learning (ML) is a field of computer science where a system can learn and improve given an objective function and the data. Mitchell gave a popular definition of machine learning in 1997: Machine learning: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. — Mitchell in 1997 (Mitchell 1997) Term Artificial intelligence (AI) is often used by the media or the general public to describe machine learning. Indeed ML can be considered as a branch of AI, together with computer vision and deep neural networks applications. However, commonly ML and AI are used interchangeably by the broad public. ML is applied commonly in many fields of science and industry. I will not discuss here subtle differences between machine learning, statistical learning, computational statistics and mathematical optimization. In general, algorithms can be divided into groups given the application: classification - aims to assign observations to a group (discrete variable) regression - aims to predict a continuous response of an input (continuous variable) clustering - aims to divide data into groups that are related to each other based on a distance Another critical distinction can be made given the inputs to the algorithm. Here, I present the differences between supervised and unsupervised learning. 2.1.1 Supervised learning Supervised learning can be described as “the analysis of data via a focused structure” (Piegorsch, n.d.). The primary task is to predict an output given the inputs. In the statistical language, the inputs are often called the predictors or the independent variables. In the pattern recognition literature, the term features are preferred. The outputs are called the responses, or the dependent variables. (Hastie, Tibshirani, and Friedman 2009) The initial data is divided into two sets: training and test. First, the model is trained with correct answers on the training data (learning to minimize the error), and then its performance is evaluated on the test data. Among widely used classifiers there are Support Vector Machines (SVM), partition trees (and their extension random forests), and neural networks. For regression, it is common to encounter linear regression, boosted trees regression, 2.1.2 Unsupervised learning In Unsupervised learning is given the data and is asked to divide the data given a particular constraint. However, the correct division of the data is not known. Therefore an unsupervised algorithm aims to unveil the “hidden structure” of the data or latent variables. One group of unsupervised learning are descriptive statistic methods, such as principal components, multidimensional scaling, self-organizing maps, and principal curves. These methods aim to represent to the data most adequately in low-dimensional space (Hastie, Tibshirani, and Friedman 2009). Another group is clustering algorithms. Clustering is the way to create groups (multiple convex regions) based on the intrinsic architecture of the data. These groups are not necessarily known beforehand but can be validated with the domain knowledge. Popular clustering algorithms are knn, k-means, hierarchical clustering. In both descriptive statistics and clustering, one important parameter (often called \\(k\\)) is number to which we want to decompose the data (number of factors, variables, clusters). Different algorithms and applications can propose an automatic choice of \\(k\\) based on formal indexes or previous knowledge, in others, the user needs to provide the \\(k\\). 2.1.3 Low-dimensional embedding for visualization There is a common confusion, often seen in computational biology, between dimension reduction and clustering. This confusion is highly pronounced with, a popular in biology, algorithm: T-distributed Stochastic Neighbor Embedding (t-SNE) (Van Der Maaten and Hinton 2008). t-SNE works in 2 main steps: (1) a probability distribution over pairs of high-dimensional objects is computed in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked, (2) t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence between the two distributions with respect to the locations of the points in the map. It is not reliable to use t-SNE for clustering as it does not preserve distances. It can also easily overfit the data and uncover ‘fake’ or ‘forced’ patterns. Therefore, a clustering should not be applied to t-sne reduced data. An alternative to the t-SNE method is recently published Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) (Mcinnes and Healy 2018)– that is based on Laplacian eigenmaps, highly scalable, reproducible and recently applied to biological data (Becht et al. 2018). Older used alternatives are ISOMAPS (non-linear dimension reduction) or PCA (Principal components analysis). For any non-linear dimension reduction method, it is not recommended to use clustering a posteriori. Clusters should be computed on original data, and then the cluster labels can be visualized in low-dimensional embedding. 2.2 Types of deconvolution One specific application of mathematical/statistical tools is deconvolution of mixed signals. According to a mathematical definition: Deconvolution : the resolution of a convolution function into the functions from which it was formed to separate their effects Alternatively, in plain English: a process of resolving something into its constituent elements or removing complication The similar problem of mixed sources can be encountered in other fields, i.e., signal processing, also known under the name of “cocktail party problem.” In the cocktail party problem, at a party with many people and music, sound in recorded with several microphones. Through blind source separation, it is possible to separate the voices of different people and the musical background (Fig. 2.1) (Cherry 1953). Figure 2.1: Illustration of the cocktail party problem. During a cocktail party voices of participants can be recorded with a set of microphones and then recovered through blind source separation. The illustration purposes only four sources are mixed with three microphones, in reality, the analysis can be performed with many sources. However, a number of samples (microphones) should be higher than the number of sources (contrary to the illustration). The same concept can be transposed to the bulk omic data, each biological species (like gene) is a cocktail party where each sample is a microphone that gathers mixed signals of different nature. The signals that form the mixtures can be different depending on the data type, and the scientific question asked. In general, the total bulk data can be split into three abundance components (Shen-Orr and Gaujoux 2013): sample characteristic (disease, clinical features) individual variation, genotype-specific or technical variation presence and abundance of different cell types expressing a set of characteristic genes Many scientists invested their efforts in order to dissect the bulk omic data into interpretable biological components. In scientific literature, there can be encountered three main understanding of tumor deconvolution: estimating clonality: using genomic data is it possible to trace tumor phylogeny raised from mutations and aberrations in tumor cells; therefore it is dissecting intra-tumor heterogeneity (i.e., using transcriptomic data (Schwartz and Shackney 2010), or more often CNA data (see Section 2.4.2) estimating purity: deconvolution into the tumor and immune/stroma compartments, often aiming to “remove” not-tumor signal from the expression data, can be performed with different data types, the most reliable estimations are usually obtained from CNA data (see Section 2.4) estimating cell-type proportions and/or profiles from bulk omics data, most of works were performed on transcriptome data (see Section 2.3) and some on the methylome data (see Section 2.4.1) These three types of deconvolution can be performed on the bulk omics data. Here we will focus on cell-type deconvolution models using bulk transcriptome. I will also briefly introduce deconvolution models applied to other data types (methylome and CNA). 2.3 Cell-type deconvolution of bulk transcriptomes The idea of un-mixing the bulk omic profiles is documented to first appear in an article of Venet et al. (2001) as a way to infer the gene expression profile of the various cell types (…) directly from the measurements taken on the whole sample In the primary hypothesis (Abbas et al. 2009), a mixture of signals from TME in transcriptomic samples can be described as a linear mixture. \\[ X = SA \\tag{2.1} \\] Where in Equation (2.1) \\(X\\) is microarray data matrix of one biological sample, \\(A\\) are mixing proportions, and \\(S\\) is the matrix of expression of genes in each cell type. Algebraically the same problem can be formalized as the latent variable model: \\[ \\forall i \\in \\{1,M\\}, \\forall j \\in \\{1,N\\} \\\\ x_{ij}= \\sum_{k=1}^K a_{kj} *s_{ik}+ e_{ij} \\tag{2.2} \\] Where \\(x_{ij}\\) is expression of gene \\(i\\) in sample \\(j\\), \\(a_{kj}\\) is the proportion of cell type \\(k\\) in sample \\(j\\) and \\(s_{ik}\\) is the expression of the gene \\(i\\) in the cell type \\(k\\), \\(K\\) total number of cell types, \\(N\\) total number of samples, \\(M\\) total number of genes. The error term \\(e_{ij}\\) cannot be directly measured. The goal of deconvolution is to reverse these equations and starting from the mixture infer the \\(A\\) (or \\(a_{kj}\\)) and \\(S\\) (or \\(s_{ik}\\)). Graphically the deconvolution of bulk gene expression can be depicted as in Fig. 2.2. Figure 2.2: Principle of the deconvolution applied to transcriptome Graphical illustration of the deconvolution of mixed samples. Starting from the left, gene expression of genes A B C is a sum of expression of cell types 1, 2, 3, 4. After deconvolution, cell types are separated, and gene expression of each cell type is estimated taking into account cell type proportions. However, in this model, either the mixing proportions, number of mixing sources or an array of specific genes need to be known. While, in the real-life case, only \\(X\\) is truly known. Therefore, developed models proposed various manners for estimating the number of mixing sources and their proportions, or the specific cell type expression. Why there is a need for cell-type deconvolution approaches? for differential gene expression analysis, to avoid confusion between a feature and cell-type abundance difference in gene expression in one cell type can be blurred by the presence of other cells expressing the gene to obtain information about a fraction of given component in the sample to infer context-specific profile or signature 2.3.1 Literature overview In order to answer general and specific need for cell-type deconvolution of bulk transcriptomes researches produced a large collection of tools. I have collected all (to my knowledge) articles published in journals or as a pre-print (up to May 2018) that propose original models/tools of cell-type deconvolution of bulk transcriptomes (Tab. 2.1). Therefore clonal deconvolution methods are not included in this overview. The transcriptome-based purity estimation methods are included as many of them proposed an initial 2-sources model that could be, at least in theory, extended to multiple sources model. Also, I did not include cell-type deconvolution methods of other data types (such as methylome). A separate (section X)[#otherDecon] is dedicated to non-transcriptome methods. The Table 2.1 contains 64 (including mine) deconvolution methods. It can be observed (Fig. 2.3) that since the begging of my thesis (2015) the number of publications has doubled (64 publications in 2018 vs. 33 in 2014). Also, from 2014 on, more methods are published every year. In Fig. 2.3 hallmark publications are indicated in red above their year of publication. The three most popular methods (based on number of citations/number of years since publication) are CIBERSORT (Newman et al. 2015) (2015, total number of citations: 343 and 88.75 citations per year), ESTIMATE (Yoshihara et al. 2013) (2013, total number of citations: 266 and 44.33 citations per year), and csSAM (Shen-Orr et al. 2010) (2010, total number of citations: 286 and 31.77 citations per year). It can be noticed that the high impact of the journal plays a role, the top 3 cited methods were published in Nature Methods and Nature Communications followed by Virtual Microdissection method (Moffitt et al. 2015) (2015) published in Nature Genetics. However, the fifth most cited publication Abbas et al. (2009) (2009, a total of 207 citations) appeared in PLOS ONE. As the index is a bit penalizing for recent publications, among commonly cited tools after 2015 are MCPcounter with 42 citations (2016, 32 without self-citations) and xCell with 14 citations (2017, 11 without self-citations). A big number of publications with a low number of citations were published in Oxford Bioinformatics or BMC Bioinformatics which underlines the importance of publishing a computational tool along with an important biological message rather than in a technical journal in order to increase a chance to be used by other researchers. Another essential aspect is the availability of the tool. One-third (in total 21) methods do not provide source code or a user-interface tool to reproduce their results. Among those articles, 13 was published before 2015. Therefore, it can be concluded that the pressure of publishers and research community on reproducibility and accessibility of bioinformatic tools gives positive results. Shen-Orr and Gaujoux (2013), authors of semi-supervised NMF method (Gaujoux and Seoighe 2012), published CellMIx: a comprehensive toolbox for gene expression deconvolution where he implements most of previously published tools in R language and group them in the same R-package. This work tremendously increased the usability of previously published deconvolution methods. The CellMix package is one of the state-of-the-art work on deconvolution that regroups algorithms, signatures and benchmark datasets up to 2013. Table 2.1: Summary of methods for cell-type deconvolution of bulk transcriptome The most popular language of implementation of published methods is R (49.2 %), followed by Matlab (11.11%), only one tool so far was published in Python. Also, most of the methods were designed to work with microarray data. There is a high chance that some of them are adaptable to RNA-seq. However, a little number of older methods was tested in a different setup. For some method, as CIBERSORT, demonstrated to work with microarray and applied commonly to RNA-seq by other researchers, the validity of results remains unclear as some studies claim that CIBERSORT performs accurately applied to RNA-seq (Thorsson et al. 2018) and other opt against it (Li, Liu, and Liu 2017; Tamborero et al. 2018). Most of newer methods (i.e. EPIC (Racle et al. 2017), quanTIseq (Finotello et al. 2017) or Infino (Zaslavsky et al. 2017)) are specifically designed for RNA-seq TPM-normalized data. Some methods, mostly enrichment-based methods, are applicable to both technologies (i.e. xCell (Aran, Hu, and Butte 2017)). It is remarkable that the general aim of the cell-type deconvolution changed with time. The earlier methods aimed to improve the power of differential expression analysis though purification of the gene expression. For example, to compare differentially expressed genes (DEG) in T-cell from the blood under two conditions. However, the obtained purified profiles from complex mixtures were often uncertain (Onuchic et al. 2016). Recently, the most mentioned goal of deconvolution is a quantification of proportions of different cell types, especially in the context of cancer transcriptomes motivated by redefinition of immunophenotypes discussed in the previous chapter. The most popular tissue of interest for deconvolution algorithms are cancer tissues and blood. Other applications are cell-cycle time-dependent fluctuations of yeast, brain cells, and glands . Mathematically speaking, I have divided methods into four categories: probabilistic, regression, matrix factorisation and convex hull depending on the nature of the approach. Most of the methods (48 - 74.6%) are working within a supervised framework, and only 20% (14) are unsupervised. The approaches will be described in detail in the following section. There are numerous practical differences between the methods. Shen-Orr and Gaujoux (2013) in their review of deconvolution tools grouped the tools depending on their inputs and outputs. Given the type of outputs, deconvolution can be considered as complete (proportions and cell profiles) or partial (one of those). Moreover, the inputs of the algorithms can be important to evaluate how practical the tool is. The most popular tools and the most recent tools ask for minimal input from the user: the bulk gene expression matrix, or even raw sequencing data (Finotello et al. 2017). Older methods usually request either at least approximative proportions of mixed cells or purified profiles to be provided. The newer methods include the reference profiles in the tool if necessary. Some tools, including most of purity estimation tools, demand an additional data input as normal samples or another data type such as CNA data (Timer (Li and Xie 2013), VoCAL (Steuerman and Gat-Viks 2016)) or image data (quanTIseq (Finotello et al. 2017)). An important parameter is also a number of sources (\\(k\\)) to which the algorithm deconvolutes the mixture. In many methods, it should be provided by the user, which can be difficult in a case of complex mixtures of human tissues. Besides, type of method can also limit the number of sources, for example, a probabilistic framework privilege lower number of sources (2-3) due to the theoretical constraints. In regression depending on provided reference the output number of estimated sources is imposed. Because of the problem of collinearity and similarity of immune cell profiles, it is hard to distinguish between cell sub-types, deconvolution into fine-grain cell subtypes is often called often deep deconvolution. Some methods (i.e., CIBERSORT, Infino, xCell) give specific attention to deconvolution of cell-subtypes. An absolute presence of a cell type in the mixture can also be an essential factor. If it is too low it can reach a detection limit, Electronic subtraction (Gosink, Petrie, and Tsinoremas 2007) discuss specifically the detection of rare cel-types. Running time and the necessary infrastructure are another way to characterize the methods. Although it is hard to compare the running time objectively simultaneously of all the tools because of the heterogeneity of methods and different datasets analysed, some tendencies can be observed. If one thinks about applying deconvolution methods to big cohorts, regression and enrichment-based methods should be well suited. As far as matrix factorisation is concerned, it depends on the implementation (i.e. R vs Matlab) and if the number of sources needs to be estimated (multiple runs for different \\(k\\) parameter) or if a stabilisation needs to applied (multiple runs for the same \\(k\\) parameter). Finally, probabilistic tools seem to be challenging to scale, i.e. authors of Infino admit that their pipeline is not yet applicable at high-throughput. In order to let user better understand the differences between different mathematical approaches, I will introduce shortly the types of approaches used for cell-type deconvolution of transcriptomes as well as their strong and weak points. Figure 2.3: Distribution of publications of cell-type deconvolution of bulk transcriptome over the years. In red: hallmark publications. Data gathered based on PubMed and google scholar search in May 2018. Figure 2.4: Simple statistics illustrating characteristics of published cell-type deconvolution tools: a) Percentage of used approach type, b) Percentage of supervised/ unsupervised tools, c) Percentage of the programming languages of implementation. Data gathered based on pubmed and google scholar search in May 2018. 2.3.2 Regression-based methods Regression models are the most popular methods for bulk gene expression deconvolution. They use estimated pure cell profiles as depending variables (or selected signature genes) that should explain the mixed profiles choosing best \\(\\beta\\) parameters (Eq. (2.3)) that can be interpreted as cell proportions. A standard type of regression is called linear regression. It reflects linear dependence between independent and dependent variables. The linear regression was developed in the precomputer age of statistics (Hastie, Tibshirani, and Friedman 2009). In linear regression, we want to predict a real-valued output \\(Y\\), given a vector \\(X^T = (X_1,X_2,… ,X_p)\\). The linear regression model has the form: \\[\\begin{equation} f(X) = \\beta_0 + \\sum_{j=1}^{p} X_j\\beta_j \\tag{2.3} \\end{equation}\\] Where the \\(\\beta_j\\)s are unknown parameters or coefficients, and \\(X_j\\)s are the explaining variables. Given pairs of (\\(x_1\\),\\(y_1\\))…(\\(x_N\\) ,\\(y_N\\) ), one can estimate coefficients \\(\\beta\\) with an optimization of an objective function (also called cost function). The most popular estimation method is least squares, the coefficients \\(\\beta = (\\beta_0, \\beta_1, ..., \\beta_n)\\) are computed to minimize the residual sum of squares (RSS): \\[\\begin{equation} RSS(\\beta) = \\sum_{i = 1}^{N}(y_i - f(x_i))^2 = \\sum_{i = 1}^{N}(y_i - \\beta_0 - \\sum_{j=1}^p x_i\\beta_j)^2 \\tag{2.4} \\end{equation}\\] Ordinary least squares regression is using Eq.(2.4) to compute \\(\\beta\\). Ridge regression (Eq.(2.5)) adds a regularizer (called \\(L2\\) norm) to shrink the coefficients (\\(\\lambda \\geq 1\\)) through imposing a penalty on their size. \\[\\begin{equation} \\hat{\\beta}^{ridge} = \\underset{\\beta}{\\text{argmin}}\\{\\sum_{i = 1}^{N}(y_i - f(x_i))^2 + \\lambda\\sum_{j=1}^{p}\\beta^2_j\\}\\tag{2.5} \\end{equation}\\] Similarly Lasso regression (Equation (2.6)) adds a regularization term to RSS (called \\(L1\\) norm), it may set coefficients to 0 and therefore perform feature selection. \\[\\begin{equation} \\hat{\\beta}^{ridge} = \\underset{\\beta}{\\text{argmin}}\\{\\sum_{i = 1}^{N}(y_i - f(x_i))^2 + \\lambda\\sum_{j=1}^{p}\\lvert\\beta_j\\rvert\\}\\tag{2.6} \\end{equation}\\] In Elastic net regression both penalties are applied. Support Vector Regression (SVR) is regression using Supported Vector Machines (SVM). In SVR \\(\\beta\\) can be estimated as follows: \\[\\begin{equation} H(\\beta,\\beta_0) = \\sum_{i=1}^{N} V (y_i − f(x_i)) +\\frac{λ}2\\lVert\\beta\\rVert^2 \\tag{2.7} \\end{equation}\\] where error is measured as follows: \\[\\begin{equation} V_\\epsilon(r) = \\begin{cases} 0, &amp; \\text{if $\\lvert r \\rvert &lt; \\epsilon$,}\\\\ \\rvert r\\lvert - \\epsilon, &amp; \\text{otherwise}. \\end{cases} \\tag{2.8} \\end{equation}\\] with \\(\\epsilon\\) being the limit of error measure, meaning errors of size less than \\(\\epsilon\\) are ignored. In the SVM vocabulary, a subset of the input data that determine hyperplane boundaries are called the support vectors (Fig.2.5). SVR discovers a hyperplane that fits the maximal possible number of points within a constant distance, \\(\\epsilon\\), thus performing a regression. In brief, in SVR, RSS is replaced by a linear \\(\\epsilon\\)-insensitive loss function and uses L2-norm penalty function. There exist variants of SVR algorithm, i.e. \\(\\epsilon\\)-SVR (Drucker et al. 1997) and \\(\\nu\\)-SVR (Schölkopf et al. 2000). \\(\\epsilon\\)-SVR allows to control the error; this favors more complex models. In the \\(\\nu\\)-SVR the distance of the \\(\\epsilon\\) margin can be controlled and therefore the number of data points used for regression can be controlled. Ju et al. (2013) used an SVM-based method to define cell type-specific genes. A model using \\(\\nu\\)-SVR with linear kernel was used by Newman et al. (2015) in CIBERSORT. Figure 2.5: Principle of the SVR regression. In SVR regression \\(\\epsilon\\) represents the limit of error measure, input data points higher than \\(+\\epsilon\\) or lower than \\(-\\epsilon\\) are called support vectors. The \\(\\nu\\) parameter in \\(\\nu\\)-SVR regression controls the distance of training error bonds: left - lower \\(\\nu\\) value larger bound, right - higher \\(v\\) margin, smaller bound. Reprinted by permission from Springer Nature (Newman et al. 2015) © 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. As unconstrained optimization of the objective function can result in negative coefficients, in the context of cell-type deconvolution, authors often aim to avoid as it complicates the interpretation. Therefore, different constraints can be imposed on the \\(\\beta\\) coefficients. The most common conditions are \\(\\beta_0 +\\beta_1+ ...+\\beta_n =1\\) and \\(\\forall\\beta_i \\geq 0\\). Solution respecting the non-negativity condition is also called non-negative least squares (NNLS) to contrast with ordinary least squares (OLS). NNLS was adopted by many authors (Venet et al. 2001; Abbas et al. 2009; Repsilber et al. 2010; Zuckerman et al. 2013; Wang et al. 2016). The task can also be solved differently from the computational perspective. Lu, Nakorchevskiy, and Marcotte (2003) and Wang, Master, and Chodosh (2006) propose to use simulated annealing to minimize the cost function. Gong et al. (2011) proposed to solve the task using quadratic programming. An extensive review on optimization of the objective function for regression methods in cell-type deconvolution was published by Mohammadi et al. (2017). Authors carefully consider different possibilities of parameter choice in the loss and regularization formulations and its performance. They present as well recommendations for construction of basis matrix and data pre- and post-processing. Digital tissue deconvolution (DTD) (Görtler et al. 2018) aims to train the loss function with in silico mixtures of single cell profiles resulting in improved performance of rare cell types (present in a small proportion). However, the training is computationally heavy, and the proper training data for bulk transcriptomes are not available. Since the publication of CIBERSORT (Newman et al. 2015) some authors (Chen et al. 2018; Schelker et al. 2017) used the Newman et al. (2015) implementation directly with pre/post modifications or different signature matrix or re-implemented the SVR regression (Chen et al. 2018). Another recent method EPIC (Racle et al. 2017) introduced weights related to gene variability. In their constrained regression, they add it explicitly in the cost function modifying RSS (Eq.(2.4)): \\[\\begin{equation} RSS^{weighted} (\\beta) = \\sum_{i = 1}^{N}(y_i - \\beta_0 - w_i \\sum_{j=1}^p x_i\\beta_j)^2 \\tag{2.9} \\end{equation}\\] with the negativity and the sum constraints, we discussed above. The \\(w_i\\) weights are corresponding to the variance of the given gene measure in the same cell type. It aims to give less importance to the variant genes. EPIC also allows a cell type that is not a referenced in the signature matrix with an assumption that the non-referenced cell type is equal to 1- a sum of proportions of other cell types (Eq.(2.10)). Authors interpret this non-referenced cell type as the tumor fraction: \\[\\begin{equation} \\beta_m = 1 - \\sum_{j=1}^{m-1}\\beta_j \\tag{2.10} \\end{equation}\\] An additional feature of EPIC is advanced data normalization and estimation of mRNA produced by each cell to adjust cell proportions, which was previously proposed by Liebner, Huang, and Parvin (2014) in the context of microarray data: \\[\\begin{equation} p_j=\\alpha \\frac{\\beta_j}{r_j} \\tag{2.11} \\end{equation}\\] where \\(p_j\\) are actual cell proportions that are ‘normalized’ with empirically derived coefficient \\(\\alpha\\) and measured \\(r_j\\) is the number of RNA nucleotides in cell type \\(j\\). Recently CIBERSORT proposed an absolute mode where the proportions are not relative to the leucocyte infiltration but to the sample. It can be obtained with an assumption that the estimation of the proportion of all genes in CIBERSORT matrix is corresponding to sample purity. This functionality was not yet officially published, and it is still in experimental phase (Newman, Liu, and Alizadeh 2018). Regression methods combined with pre- and post-processing of data can result in estimation of proportions that can be interpreted directly as a percentage of cells in a mixed sample. It is an important feature hard to achieve with other methods. Some methods provide relative proportions of the immune infiltrate (Newman et al. 2015) and another aim to provide absolute abundance (Racle et al. 2017). The absolute proportions are easily comparable between data sets and cell types. Regression-based methods are usually quite fast and can process large transcriptomic cohorts. However, as I will discuss in Validation section, they pose on the hypothesis that the reference profiles available in some context (i.e., blood) are valid in a different one (i.e., tumor) or that profiles extracted from one data type (scRNA-seq) are adapted to deconvolute bulk RNAseq. Most of the recent regression methods focused on estimating proportions and do not estimate context-specific profiles and can process as little as one sample. 2.3.3 Enrichment-based methods Table 2.2: Contingency table is the count of overlap of genes present in a certain condition (Y) vs. not present (Y-Z) and association to a pathway X (in X or not in X). The contingency table is used in the frequency based test as Fisher exact test. Y Z-Y in X a b not in X c d Enrichment-based methods aim to evaluate an amount of activity of a given list of genes within the data. This can be obtained by calculating a score based on gene expression. Traditionally enrichment methods were used to analyze set of DEG. Different statistical approaches were adapted: like Fisher exact test giving a p-value that estimated the chance a given list of genes is over/under present in the input list of DEGs and therefore characterize the condition vs. control expressed genes. Let’s take an example; if one wants to compute enrichment in pathway X of the list of DEG genes Y with the total number of tested genes Z, a contingency table need to be constructed (Tab. 2.2). In the Fisher exact test formula (Eq. (2.12)) the \\(a\\), \\(b\\),\\(c\\) and \\(d\\) are the individual frequencies, i.e. number of genes in of the 2X2 contingency table, and \\(N\\) is the total frequency (\\(a + b + c + d\\)). \\[\\begin{equation} p= \\frac{( ( a + b ) ! ( c + d ) ! ( a + c ) ! ( b + d ) ! )}{a ! b ! c ! d ! N ! } \\tag{2.12} \\end{equation}\\] Another important (&gt;14000 citations) algorithm computing such a score (enrichment score ES) is named gene set enrichment analysis (GSEA) (Subramanian et al. 2005) uses sum-statistics. The list of genes user wants to test for enrichment is usually ranked by fold change odd or p-value of DGE analysis. The high score indicated high activity of genes included in the list. GSEA can also indicate an anti-activity of correlation. A variant of GSEA, single sample GSEA (ssGSEA) (Barbie et al. 2009) was used by Şenbabaoğlu et al. (2016), Yoshihara et al. (2013) and Aran, Hu, and Butte (2017) to compute infiltration scores. In the ssGSEA genes are ranked by their absolute expression. A variance-based variant of GSEA - GSVA (Hänzelmann, Castelo, and Guinney 2013) was used by Tamborero et al. (2018) for the same purpose. MCPcounter (Becht et al. 2016) uses an arithmetic mean of gene expression of highly specific signature genes to compute a score. In this way obtained scores, are not comparable between different cell types and datasets. Therefore some authors propose normalization procedures that make the score more comparable. For instance, xCell uses a platform-specific transformation of enrichment scores. Similarly, Estimate transforms scores for TCGA though an empirically derived formula. MCPcounter authors use z-scoring to minimize platform-related differences. Unfortunately, the normalization is not directly included in the R package Even though enrichment methods do not try to fit the linear model and derived scores are not mathematically conditioned to represent cell proportions; usually there can be observed a strong linear dependence. An advantage of the enrichment-based methods is the speed and possibility to include distinct signatures that can characterize cell-types and cell-states of different pathways. 2.3.4 Probabilistic methods The probabilistic methods share a common denominator: they aim to minimise a likelihood function of Baye’s theorem: \\[\\begin{equation} p(y|\\theta) = \\frac{p(\\theta|y )* p(y)}{p(\\theta)} \\tag{2.13} \\end{equation}\\] In Eq.(2.13) \\(y\\) is our data, \\(\\theta\\) a parameter, \\(p(y|\\theta)\\) posterior, \\(p(\\theta|y )\\) likelihood and \\(p(\\theta)\\) prior. Prior distribution is what we know about the data before it was generated and combined with a probability distribution of the observed data is called posterior distribution. The likelihood describes how likely it is to observe the data (\\(y\\)) given the parameter \\(\\theta\\) (probability of \\(y\\) given \\(\\theta\\) - \\(p(y|\\theta)\\)). A parameter is characteristic of a chosen model and a hyperparameter is a parameter of prior distribution. In the literature, there are mainly different types of probabilistic models, one that assumes some type of distribution of mixed sources (i.e., Gaussian or Poisson), others that learn the distribution parameters empirically from a training set, another that try to find the parameters of the distribution given the number of given sources. Then in each case, there are different ways of constructing different priors and posteriors functions. Among used techniques are Markov Chain Monte Carlo or Expectation-Maximisation, which themselves can be implemented in different ways (Erkkilä et al. 2010; Ghosh 2004; Lähdesmäki et al. 2005; Li and Xie 2013; Roy et al. 2006, Zaslavsky et al. (2017)). The probabilistic approaches are the most popular for purity estimation (2 components models), that seems to be possible to extend to 3-components model (Wang et al. 2017). As far as cell-type decomposition into a number of cells is concerned, a method published on BioRxiv Infino uses Bayesian inference with a generative model, trained on cell type pure profiles. Authors claim their method is notably suited for deep deconvolution that is able to build cell type similarities and estimate the confidence of the estimated proportions which help to interpret the results better. A probabilistic framework is an attractive approach with solid statistical bases. It can be suited to many specific cases. The pitfalls are (1) the need of prior profiles or correct hypothesis on the distribution parameters (2) reduced performance when applied to high dimensional datasets due to extensive parameters search. 2.3.5 Convex-hull based methods An emerging family of BSS methods is convex geometry (CG)-based methods. Here, the sources are found by searching the facets of the convex hull spanned by the mapped observations solving a classical convex optimization problem (Yang et al. 2015). It can be implemented in many ways (Preparata and Shamos 1985). Convex hull can be defined as follows (Erickson, Jeff 2018): We are given a set \\(P\\) of \\(n\\) points in the plane. We want to compute something called the convex hull of \\(P\\). Intuitively, the convex hull is what you get by driving a nail into the plane at each point and then wrapping a piece of string around the nails. More formally, the convex hull is the smallest convex polygon containing the points: polygon: A region of the plane bounded by a cycle of line segments, called edges, joined end-to-end in a cycle. Points, where two successive edges meet, are called vertices. convex: For any two points \\(p\\), \\(q\\) inside the polygon, the line segment \\(pq\\) is completely inside the polygon. smallest: Any convex proper subset of the convex hull excludes at least one point in \\(P\\). This implies that every vertex of the convex hull is a point in \\(P\\). Figure 2.6: Convex hull illustration. A set of points and its convex hull (line). Convex hull vertices are black, and interior points are white. Image reproduced after Erickson, Jeff (2018). Convex hull methods have been used in many fields, from economics and engineering, I will discuss it with a focus on biological context to link tightly to cell-type deconvolution. The central assumptions of Convex hull optimization are that the gene expression of pure cell types is non-negative and that cell type proportions are linearly independent. The shapes can be fitted to a cloud of points in many ways in order to respond to given optimality criteria. A popular method introduced by Shoval et al. (2012) and applied to gene expression and morphological phenotypes of biological species employ the Pareto front concept which aims to find a set of designs that are the best trade-offs between different requirements. Visually Pareto front correspond to the edge of the convex hull. Wang et al. (2013) proposed Complex Analysis of Mixtures (CAM) method to find the Pareto front (the vertices of \\(X\\) mixed matrix (a convex set)). In the context of the cell-type deconvolution, it can be said that “the scatter simplex of pure subpopulation expressions is compressed and rotated to form the scatter simplex of mixed expressions whose vertices coincide with cell proportions”(Wang et al. 2016). In respect to the assumptions, under a noise-free scenario, novel marker genes can be blindly identified by locating the vertices of the mixed expression scatter simplex (Fa-Yu Wang et al. 2010). In the figure (Fig. 2.7), the \\(a_i\\)’s are cell-type proportions of \\(k\\) cell types, \\(s_i\\) pure cell type expression and \\(x_j\\) mixed expression in sample \\(j\\). Therefore the vertices correspond to the column vectors of the matrix \\(A\\) (Eq. (2.1)). The genes placed in a distance \\(d\\) from the vertices can be interpreted as marker genes. Figure 2.7: Fitting gene expression data of mixed populations to a convex hull shape. The geometry of the mixing operation in scatter space that produces a compressed and rotated scatter simplex whose vertices host subpopulation-specific marker genes and corresponding to mixing proportions. In the procedure suggested by Wang et al. (2013), before performing CAM, clustering (more precisely affinity propagation clustering (APC) ) is applied to the matrix in order to select genes representing clusters, called cluster centers \\(g_m\\) and dimension reduction(PCA) is applied to the sample space. Then in order to fit a convex set, a margin-of-error should be minimized. The Eq. (2.14) explains the computation of the error which computes \\(L2\\) norm of the difference between \\(g_m\\) possible vertices and remaining exterior clusters. All possibilities of combinations drew from \\(C^M_K\\), \\(M\\) number of clusters and \\(K\\) true vertices, are tested. \\[ \\text{given }\\alpha_k \\geq 0, \\sum^K_{k=1}\\alpha_k=1 \\\\ \\delta_{m, \\{1,...,K\\} \\in C^M_K }= \\underset{\\alpha_k}{min} \\sqrt{{g_m} - \\sum^K_{k=1}\\alpha_kg_k} \\tag{2.14} \\] Once optimal configuration is found, the proportions are computed using standardised averaging: \\[\\begin{equation} \\hat{\\alpha_k} = \\frac{1}{n_{markers}} \\sum_{i \\in markers} \\frac {x(i)}{\\rVert x(i)\\lVert} \\end{equation}\\] where \\(\\hat{\\alpha_k}\\) is proportion of cell type \\(k\\), \\(n_{markers}\\) number of marker genes (obtained from CAM), and \\(\\rVert x(i)\\lVert\\) is the \\(L1\\) or \\(L2\\) norm of a given marker gene \\(x_i\\). Then the cell-type specific profiles are obtained with linear regression. Authors of CAM also propose a minimum description length (MDL) index that determines the number of sources in the mixture. It selects the \\(K\\) minimizing the total description code length. So far, the published R-Java package CAM does not allow to extract gene specific signatures, and it is not scalable to large cohorts (many samples). In the article, authors apply essential pre-processing steps that are not trivial to reproduce and which are not included in their tool. Authors apply CAM and validate on rather simple mixtures (tissue in vitro mixtures and yeast cell cycle). A slightly different approach was proposed by Newberg et al. (2018). It does not require initial dimension reduction steps or clustering before fitting the convex hull, and it is based on a probabilistic framework. The toll CellDistinguisher was inspired by topic modeling algorithm (Arora et al. 2013). It first computes \\(Q\\) matrix (Eq. (2.15)). Then each row vector of \\(Q\\) is normalized to 1 giving \\(\\overline{Q}\\) matrix. Every row of \\(\\overline{Q}\\) lies in the convex hull of the rows indexed by the cell-type specific genes. Then \\(L_2\\) norm of each row is computes. Genes which rows have the highest norm can be used as distinguishers or marker genes. Then other runs of selections are applied after recentering the matrix to find more markers. \\[\\begin{equation} Q=XX^T \\tag{2.15} \\end{equation}\\] Once the set of possible distinguishers is defined, proportions and cell profiles are computed using a Bayesian framework to fit the convex hull. Authors provide a user-friendly R package CellDistinguisher. Unfortunately, they do not provide any method for estimation of some sources, which is critical for source separation of complex tissues. Additionally, quantitative weights are provided only for signature genes which number can vary for different sources, and can be as small as one gene. Authors do not apply their algorithm to complex mixtures as tumor transcriptome; they establish a proof of concept with in vitro mixtures of tissues. The convex hull-based method does not require the independence of cell types assumption, nor the non-correlation assumption which can be interesting in the setup of closely related cell types. In theory, they also allow \\(k&gt;j\\) (more sources than samples). So far, the existing tools are not directly applicable to tumor transcriptomes. 2.3.6 Matrix factorization methods Matrix factorization is a general problem not specific to cell types deconvolution. It has been extensively used for signal processing (Zinovyev et al. 2013)and extraction of features from images (Hastie, Tibshirani, and Friedman 2009). Matrix factorization can also be called BSS or dimension reduction. Despite quite simple statistical bases they have been proven to be able to solve quite complex problems. Many matrix factorization methods can solve the problem of Eq. (2.1). They can solve it in different ways and concern different hypotheses. Naturally, matrix factorization methods estimate simultaneously \\(A\\) and \\(S\\) matrices (cell proportions and profiles) given \\(X\\) rectangular matrix (genes \\(\\times\\) samples) without any additional input. Figure 2.8: Principle of matrix factorisation of gene expression. The gene expression matrix \\(X\\) is decomposed into a set of metagenes \\(S\\) matrix and metasamples \\(A\\). Number of components C is defined with parametre \\(k\\). 2.3.6.1 Principal Components Analysis One of the most popular methods, Principal Components Analysis (PCA) computes projections of the data, mutually uncorrelated and ordered in variance. The principal components provide a sequence of best linear approximations to that data. Traditionally PCA is computed through eigen decomposition of the covariance matrix. Covariance matrix is computed as follows: \\[\\begin{equation} \\Sigma = \\frac{1}{n-1}((X-\\bar{x})^T(X-\\bar{x})) \\tag{2.16} \\end{equation}\\] where \\(\\bar{x}\\) is mean vector of the feature column in the data \\(X\\). Then the matrix is decomposed to eigenvalues: \\[\\begin{equation} \\mathbf{V^{-1}\\Sigma V=D} \\tag{2.17} \\end{equation}\\] where \\(\\mathbf{V}\\) is the matrix of eigenvectors and the \\(\\mathbf{D}\\) diagonal matrix of eigenvalues. It can be also computed using singular value decomposition (SVD) (computationally more efficient way): \\[\\begin{equation} X= UDV^T \\tag{2.18} \\end{equation}\\] Here \\(U\\) is an \\(N \\times p\\) orthogonal matrix (\\(U^T U = I_p\\)) whose columns \\(u_j\\) are called the left singular vectors; \\(V\\) is a \\(p \\times p\\) orthogonal matrix (\\(V^T V = I_p\\)) with columns \\(v_j\\) called the right singular vectors, and \\(D\\) is a \\(p \\times p\\) diagonal matrix, with diagonal elements \\(d_1 \\geq d_2 \\geq ... \\geq d_p \\geq 0\\) known as the singular values. The columns of \\(UD\\) are called the principal components of \\(X\\). PCA sees’ the data as a cloud of points and finds directions in which the samples to define Principal Components. This dispersion is measured with variance, and resulting PCA components are variance-ordered. As nicely described in (Rutledge and Jouan-Rimbaud Bouveresse 2013) The first PC is the vector describing the direction of maximum sample dispersion. Each following PC describes the maximal remaining variability, with the additional constraint that it must be orthogonal to all the earlier PCs to avoid it contains any of the information already extracted from the data matrix. In other words, each PC extracts as much remaining variance from the data as possible. The calculated PCs are weighted sums of the original variables, the weights being elements of a so-called loadings vector. Inspection of these loadings vectors may help determine which original variables contribute most to this PC direction. However, PCs being mathematical constructs describing the directions of greatest dispersion of the samples, there is no reason for the loadings vectors to corresponding to underlying signals in the dataset. Most of the time, PCs are combinations of pure source signals and do not describe physical reality. For this reason, their interpretation can be fraught with danger. Especially in the context of the cell-type deconvolution, it can imagine that different cell-types contribute to the variance, but one PC could explain the joint variance of many cell types. Wang et al. (2015) used SVD to compute matrix inversion in order to separate tumor from the stroma. The method was applied to tumor transcriptomes and gives purity estimation quite different from other popular enrichment-based method ESTIMATE (Yoshihara et al. 2013). Nelms et al. (2016) in CellMapper uses a semi-supervised approach based on SVD decomposition to dissect human brain bulk transcriptome. Authors define a query gene (a specific known gene), and then they decompose transcriptome into components (eigenvectors) and multiply by weights that are higher for the components correlated with the query gene. Then the matrix is transformed back to gene \\(\\times\\) samples matrix, but query signal is amplified. The point is to find marker genes that characterize the same cell-type as the query gene. Authors did not aim at the identification of cell-type proportions or cell types profiles but identification of cell-type specific markers. They underline applicability of the method to rare cell types where many markers are not available. This approach was proposed by authors to be used to prioritize candidate genes in disease susceptibility loci identified by GWAS. 2.3.6.2 Non-negative matrix factorisation Non-negative matrix factorization (Seung and Lee 1999) is an alternative approach to principal components analysis. It assumes that data and components are non-negative. It finds its application in image analysis and gene expression analysis where data are indeed non-negative. The \\(N \\times p\\) data matrix \\(X\\) is approximated by \\[X \\approx WH \\] where \\(W\\) is \\(N \\times r\\) and \\(H\\) is \\(r \\times p\\), \\(r ≤ max(N,p)\\). We assume that \\(x_{ij}\\) ,\\(w_{ik}\\), \\(h_{kj}\\)\\(\\geq 0\\). Which is a special case of Eq. (2.1). The matrices \\(W\\) and \\(H\\) are found by maximizing \\[\\begin{equation} \\mathcal{L}(W, H) = \\sum^N_{i=1}\\sum^{p}_{j=1}[x_{ij} log(WH)_{ij} − (WH)_{ij} ] \\tag{2.19} \\end{equation}\\] This is the log-likelihood from a model in which \\(x_{ij}\\) has a Poisson distribution with mean \\((WH)_{ij}\\). This formula can be maximized through minimization of divergence: \\[\\begin{equation} \\underset{W,H}{min} f(W,H) = \\frac{1}{2}\\rVert X-WH\\rVert^2_F \\tag{2.20} \\end{equation}\\] Where \\(\\rVert .\\rVert_F\\) is Frobenius norm, which can be replaced by Kullback-Leibler divergence. The optimization can be done employing different methods: euclidean update with multiplicative update rules, it is the classic NMF (Seung and Lee 1999) \\[ W \\leftarrow W \\frac{XH^T}{WHH^T}\\\\H \\leftarrow H\\frac{W^TX}{W^TWH} \\tag{2.21} \\] alternating least squares (Paatero and Tapper 1994) where the matrices W and H are fixed alternatively alternating non-negative least squares using projected gradients (Lin (2007)) convex-NMF (Ding, Tao Li, and Jordan 2010) imposes a constraint that the columns of \\(W\\) must lie within the column space of \\(X\\), i.e. \\(W = XA\\) (where \\(A\\) is an auxiliary adaptative weight matrix that fully determines \\(W\\)), so that \\(X=XAH\\). In this method only \\(H\\) must be non-negative. The NMF algorithms can differ in initialization method as well and even in situations where \\(X = WH\\) holds exactly, and the decomposition may not be unique. This implies that the solution found by NMF depends on the starting values. The performance of different combinations applied to MRS data from human brain tumors can be found in Ortega-Martorell et al. (2012). Brunet et al. (2004) created an NMF Matlab toolbox and demonstrated applicability of NMF (using Kullback-Leibler divergence and euclidean multiplicative update (Lee and Seung 2000) to cancer transcriptomes with focus on cancer subtyping (focusing on the \\(H\\) matrix). Brunet et al. (2004) also proposed a way to evaluate the optimal number of factors (sources) to which matrix should be decomposed. NMF as imposing non-negativity in the context of decomposition of transcriptomes seems as an attractive concept as both cell profiles and cell proportions should be non-negative. It is not surprising then that some authors used NMF to perform cell-type deconvolution. To my knowledge, deconf (Repsilber et al. 2010) was the first tool proposing NMF cell-type deconvolution of PBMC transcriptome, of considerable dimensions, 80 samples (40 control and 40 cases) of Tuberculosis. Repsilber et al. (2010) employed random initialization and alternating non-negative least squares to minimize the model divergence. The complete deconvolution of the transcriptome was used to perform DEG analysis on the deconvoluted profiles. Shen-Orr and Gaujoux (2013), not only presented exhaustive literature review through implementing cell-type deconvolution methods in an R package CellMix (Gaujoux and Seoighe 2013) but also proposed a semi-supervised NMF for cell-type deconvolution and published an R package implementing different NMF methods (Gaujoux and Seoighe 2010). The semi-supervised version of NMF proposed by Gaujoux and Seoighe (2013), need a set of specific marker genes for each desired cell type. Then at initialization and after each iteration of the chosen NMF algorithm (applies to some versions of NMF (Seung and Lee 1999; Brunet et al. 2004; Pascual-Montano et al. 2006), “each cell type signature has the values corresponding to markers of other cell types set to zero. The values for its own markers are left free to be updated by the algorithm’s own iterative schema”. Applying their algorithm to in vitro controlled dataset [GSE11058 (Abbas et al. 2009), testing selected NMF implementations and a varying number of markers per cell, authors observed the best performance with guided version of brunet (Brunet et al. 2004) implementation. Moffitt et al. (2015) applied NMF to separate tumor from stroma in pancreatic ductal carcinoma (PDAC) using multiplicative update NMF. They scaled \\(H\\) matrix rows to 1 so that the values correspond to the proportions. Authors tested different possibilities of the number of sources (\\(k\\)), the final number of factors was defined through hierarchical clustering on gene-by-gene consensus matrix of top 50 genes of each component. Finally Liu et al. (2017) proposed post-modified NMF in order to separate in vitro mixtures of different tissues. In brief, NMF is a popular, in biology, algorithm performing source separation with non-negativity constraint. It was applied to in vitro cell-mixtures and blood transcriptomes, showing a satisfying accuracy of cell-type in silico dissection and evaluating proportions. It was also applied in cancer context. However, it did not recover cell-type specific signals but rather groups of signals that could be associated with cancer or stroma. 2.3.6.3 Independent Components Analysis Independent Components Analysis is written as in Eq. (2.1) with assumption that columns of \\(S\\): \\(S_i\\) are independent and non-Gaussian, which adds orthogonality condition to \\(A\\), since \\(S\\) also has covariance \\(I\\). It was first formulated by Herault and Jutten (1986) The independence can be measured with entropy, kurtosis, mutual information or negentropy measure \\(J(Y_j )\\) (Hyvärinen and Oja 2000) defined by \\[\\begin{equation} J(Y_j ) = H(Z_j ) − H(Y_j ) \\tag{2.22} \\end{equation}\\] where \\(Z_j\\) is a Gaussian random variable with the same variance as \\(Y_j\\). Negentropy is non-negative and measures the deviation of \\(Y_j\\) from Gaussianity. It is used in a popular implementation of FastICA (Hyvärinen and Oja 2000). Other existing implementations of ICA are Infomax (Bell and Sejnowski 1995) using Information-Maximization that maximizes the joint entropy JADE (Cardoso and Souloumiac (1993)) on the construction of a fourth-order cumulants array from the data However, they are usually a lot slower which limits their application to a large corpus of data and Teschendorff et al. (2007) demonstrated that FastICA gives the most interpretable results. Therefore, I will focus on FastICA implementation as it will be extensively used in the Results part. FastICA requires prewhitening of the data (centering and whitening). Centering is removing mean from each row of \\(X\\) (input data). Whitening is a linear transformation that columns are not correlated and have variance equal to 1. Prewhitenning Data centering \\[\\begin{equation} x_{ij} \\leftarrow x_{ij} - {\\frac {1}{M}}\\sum_{j^{\\prime }}x_{ij^{\\prime }} \\tag{2.23} \\end{equation}\\] \\(x_{ij}\\): data point Whitenning \\[\\begin{equation} \\mathbf {X} \\leftarrow \\mathbf {E}\\mathbf {D} ^{-\\frac{1}{2}}\\mathbf {E} ^{T}\\mathbf {X} \\tag{2.24} \\end{equation}\\] Where \\(\\mathbf {X}\\) - centered data, \\(\\mathbf {E}\\) is the matrix of eigenvectors, \\(\\mathbf{D}\\) is the diagonal matrix of eigenvalues Algorithm 1: FastICA multiple component extraction INPUT: \\(K\\) Number of desired components INPUT: \\(X \\in \\mathbb{R}^{N \\times M}\\) Prewhitened matrix, where each column represents an \\(N\\)-dimensional sample, where \\(K \\leq N\\) OUTPUT: \\(A \\in \\mathbb{R}^{N \\times K}\\) Un-mixing matrix where each column projects \\(\\mathbf {X}\\) onto independent component. OUTPUT: \\(S \\in \\mathbb{R}^{K \\times M}\\) Independent components matrix, with \\(M\\) columns representing a sample with \\(K\\) dimensions. \\[\\begin{equation} \\tag{2.25} \\end{equation}\\] For \\(p\\gets [1, K]\\) \\(\\mathbf{w}_{p} \\gets\\) Random vector of length \\(N\\) WHILE \\(\\mathbf{w_{p}}\\) changes \\(\\mathbf{w}_p \\gets \\frac{1}{M}Xg(\\mathbf{w}_p^TX)^T - \\frac{1}{M}g&#39;(\\mathbf{w}_p^TX)1w_p\\) \\(\\mathbf{w}_p \\gets \\mathbf{w}_p - (\\sum_{j=1}^{p-1}\\mathbf{w}_p^T\\mathbf{w}_j\\mathbf{w}_j^T)^T\\) \\(\\mathbf{w}_p \\gets \\frac{\\mathbf{w}_p}{\\lVert \\mathbf{w}_p \\rVert}\\) where \\(\\mathbf {1}\\) is a column vector of 1’s of dimension \\(M\\) OUTPUT: \\(A = [\\mathbf{w}_1, ..., \\mathbf{w}_K]\\) OUTPUT: \\(S = \\mathbf{A}^T\\mathbf{X}\\) However, the results of this algorithm (Alg. 1. (2.25)) are not deterministic, as the \\(\\mathbf{w}_{p}\\) initial vector of weights is generated at random in the iterations of fastICA. If ICA is run multiple times, one can measure stability of a component. Stability of an independent component, regarding varying the initial starts of the ICA algorithm, is a measure of internal compactness of a cluster of matched independent components produced in multiple ICA runs for the same dataset and with the same parameter set but with random initialization (Himberg and Hyvarinen, n.d.). The Icasso procedure can be summarized in a few steps : applying multiple runs of ICA with different initializations clustering the resulting components defining the final result as cluster centroids estimating the compactness of the clusters In brief, ICA looks for a sequence of orthogonal projections such that the projected data look as far from Gaussian as possible. ICA starts from a factor analysis solution and looks for rotations that lead to independent components. So far, ICA was used to deconvolute transcriptomes into biological functions (Biton et al. 2014; Engreitz et al. 2010; Gorban 2007; Teschendorff et al. 2007; Zinovyev et al. 2013). However, it has never been used for cell-type deconvolution. In theory, ICA outputs: \\(S\\) could be interpreted as sources and \\(A\\) as proportions in the cell-type deconvolution context. In practice, the fact that the ICA allows negative weights of projections, it makes the interpretation less trivial. To my knowledge, my DeconICA R-package (that will be described in the results part) is the first method allowing interpretation of ICA-based signals as cell-type context-specific signatures and quantify their abundance in the transcriptome. Figure 2.9: Simple illustration of matrix factorisation methods. Adapted with permision from (Zinovyev et al. 2013) All in all, matrix factorization methods are able to decompose a gene expression matrix into a weighted set of genes (metagene)(\\(S\\)) and weighted set of samples (metasample \\(A\\)). Discussed here PCA, NMF and ICA differ in constraints and starting hypotheses. PCA components are ordered by variance and are orthogonal in the initial space of data (Fig. 2.9 ). NMF impose non-negativity constraint and ICA independence of sources hypothesis. NMF and ICA do not have a particular order. For all the matrix factorization methods number of components (or factors) (\\(k\\)) needs to be given to the algorithm. Some authors propose a way to estimate the optimal number of components usually justified in a specific context. NMF and SVD were applied in the context of cell-type deconvolution while ICA, so far, was used to dissect transcriptome into factors related to signaling pathways, technical biases or clinical features. Also, ICA was proven to find reproducible signals between different datasets (Cantini et al. 2018; Teschendorff et al. 2007). I am going to discuss this aspect of the [Results] section. 2.3.7 Attractor metagenes A method proposed by Cheng, Yang, and Anastassiou (2013) that can be run in semi-supervised or unsupervised mode is called attractor metagenes. Authors describe their rationale as follows: We can first define a consensus metagene from the average expression levels of all genes in the cluster, and rank all the individual genes in terms of their association (defined numerically by some form of correlation) with that metagene. We can then replace the member genes of the cluster with an equal number of the top-ranked genes. Some of the original genes may naturally remain as members of the cluster, but some may be replaced, as this process will “attract” some other genes that are more strongly correlated with the cluster. We can now define a new metagene defined by the average expression levels of the genes in the newly defined cluster, and re-rank all the individual genes concerning their association with that new metagene; and so on. It is intuitively reasonable to expect that this iterative process will eventually converge to a cluster that contains precisely the genes that are most associated with the metagene of the same cluster so that any other individual genes will be less strongly associated with the metagene. We can think of this particular cluster defined by the convergence of this iterative process as an “attractor,” i.e., a module of co-expressed genes to which many other gene sets with close but not identical membership will converge using the same computational methodology. Which in pseudocode works as described in Algorithm 2 (2.26) and it is implemented in R code is available online in Synapse portal. Algorithm 2: Attractor metagenes algorithm INPUT: \\(\\alpha\\) shrinkage parameter INPUT: \\(X \\in \\mathbb{R}^{N \\times M}\\) gene expression matrix OUTPUT: \\(m_j\\) metagene of \\(g_{seed}\\) \\[\\begin{equation} \\tag{2.26} \\end{equation}\\] \\(g_{seed} \\gets\\) a gene from \\(1:N\\) \\(I^{\\alpha}(g_{seed}; g_i)\\) # compute association beteen \\(g_{seed}\\) and \\(g_i\\) \\(w_i = f(I^{\\alpha}(g_{seed}; g_i))\\) # compute weights for each gene \\(m_0 = \\frac{\\sum^N-1_{i=1}(g_iw_i)}{\\sum^N-1_{i=1}w_i}\\) # compute metagene as weighted average of all genes \\(I^{\\alpha}(m_0; g_i)\\) # compute association between metagene \\(m_0\\) and each gene \\(g_i\\) REPEAT \\(w_i = f(I^{\\alpha}(m_0; g_i))\\) \\(m_j = \\frac{\\sum^N-1_{i=1}(m_0w_i)}{\\sum^N-1_{i=1}w_i}\\) UNTIL \\(m_{j+1} = m_j\\) The produced signatures’ weights are non-negative. In the original paper, the generation of tumor signatures leads to three reproducible signatures among different tumor types, including leucocyte metagene. Typically with the essential parameter \\(\\alpha=5\\), they discovered typically approximately 50 to 150 resulting attractors. Attractor Metagenes algorithm can be seen as a variant of clustering approach where distance metric is mutual information between genes and metagenes are weighed average of gene expression. This method was further to study breast cancer (Al-Ejeh et al. 2014) and to SNP data (Elmas et al. 2016). There is a possibility to tune the \\(\\alpha\\) parameter in order to obtain more or less metagenes that would be possibly interpretable as cell-type signatures. 2.3.8 Others aspects Here I will discuss transversal aspects common to most deconvolution methods. They play the critical role in the final results and are often omitted while algorithms are published which impacts the reproducibility significantly. 2.3.8.1 Types of biological reference Let us consider the most common case of the deconvolution where neither \\(A\\) or \\(S\\) are not known (Eq. (2.1)), and we would like to estimate cell proportions or both cell proportions and cell profiles. No matter if the method is supervised or unsupervised at some point of the protocol the biological knowledge about cell types is necessary in order to either derive the model or interpret the data. I discussed signatures from the biological perspective in Section X. Here, I would like to stress the importance of the design of gene signatures which aim is to facilitate cell-type deconvolution. Depending on chosen solution different type of reference can be used. In regression algorithms, a proxy for purified cell profiles is necessary to estimate proportions. However, the genes that are the most variant between cell types are enough for regression, and not all profiles are necessary. The choice of the genes and the number of the genes impact the outcome (Vallania et al. 2017) significantly. Therefore, most of the regression methods come together with a new basis matrix, ranging from hundreds to tens of genes. Typically, genes selected for basis matrix should be cell-type specific in respect to other estimated cell types, validated across many biological conditions (Hoffmann et al. 2006). Racle et al. (2017) adds a weight directly in the regression formula (see Eq. (2.9) ) that corresponds to the variability of a signature gene between independent measurements of the same cell type so that the least inter-cell type variable genes have more weight in the model. CellMix (Gaujoux and Seoighe 2013) regroups different indexes to select the most specific genes based on signal-to-noise ratio. However, the most popular method is the selection of differentially expressed genes between pure populations. Often criteria for the optimal number of genes in the basis matrix are not knowledge-based but data-driven. Abbas et al. (2009) uses condition number of basis matrix (kappa) in order to select the number of genes. CIBERSORT and many other regression methods follow the same approach. Newman et al. (2015) also added another step while constructing the basis matrix, and it preselects reference profiles having maximal discriminatory power. Some authors (Ju et al. 2013; Nelms et al. 2016) propose to find marker genes though correlation with a provided marker gene (a single one or a group of genes). In enrichment methods, gene list can be enough to estimate cell abundance, sometimes (i.e., GSEA) ranked gene list is necessary. The choice of extremely specific markers is crucial for accurate call-type abundance estimation. The choice of markers can also be platform-dependent, this point is strongly underlined in (Becht et al. 2016). An interesting possibility is the use of gene list of different cell states in order obtain coarse-grain resolution. The impact of missing gene from a signature in the bulk dataset remains an unanswered question. It would be logical that shorter the gene list for a specific cell, a lack of a gene can have more impact on the result. There is a need for an accurate threshold between robustness and accuracy of the method. In unsupervised methods, purified cell-profiles, signatures or list of genes can be used a posteriori to interpret the obtained sources. Even though the choice of reference does not affect the original deconvolution, it affects the interpretation. The advantage of a posteriori interpretation is a possibility to use different sources and types of signatures in order to provide the most plausible interpretation. It is common that the way of interpretation of components is not included in the deconvolution tool (Wang et al. 2016, Newberg et al. (2018), Moffitt et al. (2015)), even though it is a crucial part of the analysis. For the deconvolution of tumoral biopsies, most of the reference profiles, up to now, are coming from the blood, which is the most available resource. Therefore most of the methods make a hypothesis that blood cell-type profiles/signatures are a correct approximation of cancer infiltrating immune cells. Rare models like PERT (Qiao et al. 2012) or ImmuneStates (Vallania et al. 2017) discuss the perturbation of the blood-derived profiles in diseases. With the availability of single-cell RNA-seq of human cancers (Chung et al. 2017; Lavin et al. 2017; Li, Liu, and Liu 2017; Puram et al. 2017; Schelker et al. 2017; Tirosh et al. 2016; Zheng et al. 2017), we gain more knowledge on immune cells in TME, and there is growing evidence that they differ importantly from blood immune cells. Racle et al. (2017) show that lymph node-resident immune cells have expression profile closer to blood immune cells than cancer immune cells. Schelker et al. (2017) shows, using a synthetic bulk dataset that using single cell profiles with existing regression methods (CIBERSORT) can improve their performance in the cancer context. However, availability of scRNA-seq remains succinct and probably do not embrace the patient heterogeneity that can be found in large bulk transcriptome cohorts. 2.3.8.2 Data normalization Data pre- and post-processing can have a substantial impact on the deconvolution. Many authors apply strong filtering of genes (Wang et al. 2016), removing probes with a low and moderate expression as well as genes with the highest expression (potential outliers). In many cases, data preprocessing is not detailed and therefore impossible to reproduce. There is also a debate on the data normalization. Most of the authors suggest to use counts (not log-space) for estimating cell abundance as log) transformed data violate the linearity assumption (Zhong et al. 2013), some opt against it (Shannon et al. 2017; Clarke, Seo, and Clarke 2010), and some envisage both possibilities (Erkkilä et al. 2010; Repsilber et al. 2010). For the RNA-seq data TPM (transcripts per million) normalization is preferred or even required by most methods (Chen et al. 2018; Finotello et al. 2017; Racle et al. 2017). 2.3.8.3 Validation Most of algorithm validation starts with in silico mixtures (Fig. 2.10). In published articles, the bulk transcriptome is simulated in two ways (1) mixing numerically simulated sources at defined proportions of given distribution (i.e. uniform) using linear model (for instance NMF) (2) using sampling (for instance Monte Carlo) to randomly select existing pure profiles and mixing them (additive model) at random proportions. To the obtained bulk samples, noise can be added at different steps of the simulation. Additional parameters can be defined in in silico mixtures, for instance, CellMix allows defining the number of marker genes (specific to only one source) for each cell type. The simulated benchmark based on single cell data was used in Schelker et al. (2017) and Görtler et al. (2018). In this framework, simulated data was obtained through summing single cell profiles at known proportions. The main pitfall of those methods is that in the proposed simulations the gene covariance structure is not preserved. In reality, the proportions of cell types are usually not random, and some immune cell types can be correlated or anti-correlated. In addition, these simulations create a simple additive model which perfectly agrees with the linear deconvolution model. This is probably not the case of the real bulk data affected by different factors as cell cycle, technical biases, patients heterogeneity and especially cell-cell interactions. Figure 2.10: From theory to practice: a simplified pipeline of model validation. The scheme reflects pipeline of data validation commonly used in transcriptome deconvolution methods validation. The project can be started from a biological problem (A) and then a way to solve the problem in mathematical model (C) is tested. Most commonly, the project starts with the model (C) then it is tested on simulated data (D). Next level of difficulty is testing the model with real data, so-called, benchmark data (B) that were generated in some biological context different from initial problem. They need to be usually normalized (E) before the model is challenged. B data are widely used as they are easily available and there is some validation available facilitating comparison. Lastly, it is assumed that if the method works fine in the context B, it will work as well in the context A, preferably accompanied by some partial validation. One can replace A by cancer transcriptomics, B by blood data or in vitro mixtures if the focus is TME bulk transcriptomics deconvolution. Naturally, algorithms validated with simulated mixtures are then validated with controlled in vitro mixtures of cell types or tissues mixed in known proportions. The most popular benchmark datasets are: mix of human cell lines Jurkat, THP-1, IM-9 and Raji in four different concentration in triplicates and the pure cell-line profiles (GSE11058) (Abbas et al. 2009); mix of rat tissues: liver, brain, lung mixed in 11 different concentrations in triplicates and the pure tissues expression(GSE19830) (Shen-Orr et al. 2010) Similar simple mixtures are also proposed by other authors (Becht et al. 2016, Kuhn et al. (2011)). This type of benchmark adds the complexity of possible data processing and experimental noise. However, it still follows an almost perfect additive model as the cell/tissues do not interact and they are only constituents of the mixture. Several tools performed systematic benchmark using PBMC or whole blood datasets, where for a number of patients (that can be over one hundred) FACS measured proportions of selected cell types and bulk transcriptomes are available. Many such datasets can be found at IMMPORT database. Aran, Hu, and Butte (2017) kindly shared with scientific community two datasets with a considerable number of patients (\\(\\sim80\\) and \\(\\sim110\\)) and processed FACS data (actual proportions) on their github repository. It is still important to remember that liquid tissues are easier to deconvolute and for the tools using a priori reference, the reference profiles are obtained from the blood. Therefore the context remains consistent. For the solid cancer tissues deconvolution, some of the tools were validated with the stained histopathology cuts using in situ-hybridisation (ISH) (Kuhn et al. 2011) or immunohistochemistry (IHC) (Becht et al. (2016)). Often this method estimates a limited number of cell types and the measured abundance of pictures can also be biased by the technical issues (image/ staining quality). Authors of EPIC validated their tool with paired RNAseq and Lymph node FACS-derived proportions in 4 patients (GSE93722). They also noticed that it is more straightforward to correctly evaluate lymph node immune cell types than cancer infiltrating cell types as lymph node-resident cells are more similar to the blood immune cells. FACS and gene expression of blood cancer (Follicular lymphoma) were also used by Newman et al. (2015) for 14 patients (unpublished data). For solid tissues, Newman et al. (2015) used paired FACS and expression datasets of normal lung tissues for B-cell and CD8 and CD4 T cells of 11 patients (unpublished data). Some authors proposed to cross-validate estimated proportions with estimated proportions based on a different data input (i.e., methylome) (Li et al. 2016, Şenbabaoğlu et al. (2016)) or CNA (Şenbabaoğlu et al. (2016)). This type of validation is interesting, even though in many projects only one type of data types are available for the same samples. TCGA data is one of few exceptions. Finally, a validation of deconvolution of solid cancer tissues remains incomplete as no paired expression and FACS data is available up to date. 2.3.8.4 Statistical significance A little number of tools propose a statistical significance assessment. CIBERSORT computes empirical p-values using Monte Carlo sampling. Infino authors (Zaslavsky et al. 2017) provide a confidence interval for the proportion estimations. This allows knowing which proportion estimation are more trustful than other. Most tools compare themselves to others measuring the accuracy of the prediction, or Pearson correlation, on the benchmark datasets (described above). Often, in the idealized mixtures, methods perform well. Evaluation of their performance in cancer tissues remains unanswered without proper statistical evaluation. 2.3.9 Summary The field of computational transcriptome deconvolution is continuously growing. Initially used to solve simple in vitro or simulated mixtures of quite distinct ingredients, then to deconvolute blood expression data, finally applied to solid cancer tissues. In cancer research, digital quantification of cancer purity becomes a routine part of big cancer research projects (Yoshihara et al. 2013). Cell-type quantification, even though the validation framework and statistical significance of deconvolution tools can still be improved, seems to be considered as a popular part of an analytical pipeline of bulk tumor transcriptomes (Cieślik and Chinnaiyan 2017). Different types of approaches try to solve the deconvolution problem, focusing on different aspects of the quantification, or proposing methodologically different approaches. Methods proposing an unsupervised solution to the deconvolution problem of transcriptomes are still underrepresented. All the tools assume a linear additive model without explicitly including the impact of possible interactions on the cell-type quantification. The tools that met the most prominent success were proven by the authors to be readily applicable to a variety of cancer datasets and reusable without an extra effort (through a programming library or web interface). The field is still waiting for a gold standard validation benchmark that would allow a fair comparison of all the tools in solid cancer tissues. It is also remarkable that the recent methods focus on quantification of the abundance of an average representation of cell-types without aspiring to deconvolute the cell-type context-specific profiles. Thanks to various cancer single-cell data and big-scale projects (Regev et al. 2017), we will be able to improve the existing deconvolution approaches and finally replace the collection of bulk transcriptomes by a collection of scRNA-seq ones. 2.4 Deconvolution of other data types The transcriptome data is not the unique omic data type that can be used to infer cell type proportions. Genomic and epigenomic data was used in numerous publications to perform cell-type deconvolution or estimate sample purity. I will present a general landscape of the tools and methods used for this purpose. 2.4.1 DNA methylation data Cell-type composition can be computed from DNA methylation data (described in Section X). In EWAS (Epigenome Wide Association Studies) variation origination from cell types is considered as an important confounding factor that should be removed before comparing cases and controls and defining Differentially Methylated Positions (DMPs). Teschendorff and Zheng (2017) reviewed ten tools for epigenome deconvolution. Authors identify six of the described methods as reference-free (which I called in this Chapter unsupervised), three are regression-based, and one is semi-supervised. Another review on this topic was authored by Titus et al. (2017). Unsupervised methods employed in methylome cell-type deconvolution are RefFreeEWAS (Houseman, Molitor, and Marsit 2014), SVA (Leek and Storey 2007) are based on SVD, ISVA based on ICA [Teschendorff, Zhuang, and Widschwendter (2011)) are more general methods that aim to detect and remove confounders from the data (that do not need to be necessary the cell types). RUVm (Maksimovic et al. 2015) is a semi-supervised method using generalized least squares (GLS) regression with negative reference also used to remove unwanted variation from the data and could be potentially adapted to cell-type deconvolution. EWASher (Zou et al. (2014)) is linear mixed model and PCA based method that corrects for cell-type composition. Similarly, ReFACTor (Rahmani et al. 2016) use sparse PCA to remove the variation due to cell-type proportions. Houseman et al. (2016) proposed RefFreeCellMix: an NMF model with convex constraints to estimate factors representing cell types and cell-type proportions and a likelihood-based method of estimating the underlying dimensionality (\\(k\\) number of factors). A different tool MeDeCom (Lutsik et al. 2017) uses alternating non-negative least squares to fit a convex hull. As far as supervised methods are concerned, EPiDISH (Epigenetic Dissection of Intra-Sample-Heterogeneity) R-package (Teschendorff and Zheng 2017) includes previously published tools: Quadratic programming method using reference specific DNAse hypersensitive sites [Constrained Projection (CP) (Houseman et al. 2012)), adapted to methylome deconvolution CIBERSORT algorithm (\\(\\nu\\)-SVR) and robust partial correlations (RPC) method (a form of linear regression). Reference cell-type specific profiles were obtained from the blood. eFORGE (Breeze et al. 2016) can detect in a list of DMPs if there is a significant cell-type effect. EDec (Onuchic et al. 2016) uses DNAm to infer factors proportions using NMF and then derives factors profiles though linear regression of transcriptome data of cancer datasets. Authors identify the tumor and stroma compartments and profiles. However, they admit the error rate for profiles is quite high for most genes. Wen et al. (2016) focused on intra-tumor heterogeneity (clonal evolution) based on DNAm data. Profiles obtained from cell lines were used in a regression model to identify the proportions of sub-clones in breast cancer data. InfiniumPurify (Zheng et al. 2017) and LUMP (Aran, Sirota, and Butte 2015) uses DNAm to estimate sample purity. The validation framework for methylation deconvolution is very similar to transcriptome ones: in silico mixtures and FACS-measured proportions of the blood. Most of the tools assume the cell composition is a factor the most contributing to the variability and therefore SVD/PCA based approaches are sufficient to correct for the variability. According to Teschendorff and Zheng (2017), this assumption was not proven to hold true in solid tissues like cancer. Supervised methods have the same drawback as in the case of the transcriptome, and they use purified profiles from one context to derive cell proportions in a different context. In overall, it seems that no study proposed cell-type quantification based on methylome profiles in a pan-cancer manner. 2.4.2 Copy number aberrations (CNA) To my knowledge there is no method using CNA data in order to estimate cell-type composition, as CNA occur in tumor tissue and natural distinction can be made between tumor and normal cells and within tumor cells (intra-tumor). Therefore, copy number aberrations can be used to estimate tumor purity and clonality. BACOM 2.0 (Fu et al. 2015), ABSOLUTE (Carter et al. 2012), CloneCNA (Yu, Li, and Wang 2016), PureBayes (Larson and Fridley 2013), CHAT (Li and Li 2014), ThetA (Oesper, Mahmoody, and Raphael 2013), SciClone (Miller et al. 2014), Canopy (Jiang et al. 2016), PyClone (Roth et al. 2014), EXPANDS (Andor et al. 2014) estimate tumor purity and quantify true copy numbers. OmicTools website reports 70 tools serving this purpose and their review goes beyond the scope of my work. Most tools use tumor and normal samples, paired if possible. QuantumClone seem to be the only tool that requires a few samples from the same tumor (in time or space dimension). Aran, Sirota, and Butte (2015) published Consensus measurement of purity estimation that combines purity estimations based on different data types (available in cBioportal) using: ESTIMATE (Yoshihara et al. 2013) (gene expression data), ABSOLUTE (Carter et al. 2012) (CNA), LUMP (Aran, Sirota, and Butte 2015) (DNAm and IHC of stained slides. Authors concluded that the estimation based on different data types highly correlate with each other, besides the IHC estimates, which suggests that IHC provides a qualitative estimation of purity. 2.5 Summary of the chapter A plethora of machine learning solutions has been developed to solve problems of different nature. Supervised and Unsupervised approaches can be distinguished depending if a model is provided a set of training data with known response or the algorithm works blindly trying to find patterns in the data. Some of the algorithms found an essential application in healthcare and are included in the clinical routine. One of the critical problems that can, in theory, be solved with ML, is bulk omic data deconvolution. Different types of deconvolution of cancer samples can be distinguished: clonal, purity and cell-type deconvolution. Here I focused on cell-type deconvolution of transcriptome data. Through an extensive review, I presented 64 tools and divided them into categories depending on the adapted type of approach. I distinguished probabilistic, enrichment-based, regression-based, convex hull, matrix factorization and attractor metagene approaches that can be used for cell-type deconvolution. I detailed the basis of the different models and highlighted the most important features counting for cell-type deconvolution. DNAm data were also used to estimate cell-type proportions. However, the heterogeneity found in methylome data resulting from the difference in cell type proportions is usually seen as a confounding factor to be removed. CNA data can be used for estimation of tumor purity and clonality. In brief, for the transcriptome cell-type deconvolution, it can be observed that just a limited number of tools are usable in practice in order to deconvolute large cancer cohorts and without the need to provide hard to estimate parameters. Supervised methods applied to cancer use reference-profiles coming from a different context. Unsupervised tools, so far, are somewhat underrepresented in the field and do not offer a solution directly applicable to cancer transcriptomes of high dimensions. All of the presented methods are still waiting for consistent validation with the gold standard benchmark. This could be done if systematic data of bulk transcriptome paired with FACS-measured cell-type proportions information for many cells and in many samples were generated. Another unanswered question is the validity of the linear mixture model in the presence of cell-cell interactions. There is still a room for improvement in the field in order to provide more user-friendly, accurate and precise cell-type abundance estimations. A question can be asked, are cell-type proportions enough to understand tumor immune phenotypes? Can we extract more valuable information from the bulk omic data that would give useful insight to biological functions of the in silico dissected cells? References "],
["objectives.html", "Objectives", " Objectives In the introduction, I have described two sides of studying TME complexity. I placed in the context of cancer research and cancer therapy the most recent studies of tumor immunity with a focus on system-level computational approaches. I have also introduced a wide array of available approaches to address deconvolution of bulk omic data. I reviewed their strong and weak points, and I presented general trends since the field was established. Answers to important questions on how TME modulates tumor, how to propose better cancer subtyping for immune therapies and how to predict better response to treatment are perhaps hidden in already generated bulk omic data. However, new methodological tools and a more overall view is needed to uncover hidden patterns better. In this thesis, I aim to bring new insights into composition and function of TME. It is clear that complex information is necessary to understand the role of different immune cells in cancer and not only presence but also function are to be deciphered from available data. Therefore, this project, on its biological side, has two main aims: fundamental research: understand the presence of different cell type, their interactions and functions in TME of different cancers types and how other factors as stress, cell cycle, etc. shape them. Thanks to data-driven and discovery nature of the project, I will also hope to understand how the signature of cell type evolves in different conditions shaped by other cells and factors. translational research: how immune landscape and its state can help to predict patient survival and better tailor recommendation for therapy. The analysis could also bring to the light possible biomarkers or drug targets for immune therapies. I aim to explore publicly available data, challenge inter-lab, and inter-platform biases. I will use mainly bulk transcriptomic data (because of available volume) and cross-validated with other data types: scRNA-seq, FACS, IHC when possible. On its computational/mathematical side it will face following challenges: Establish state-of-art of existing bulk deconvolution methods, discuss their advantages and limits Propose new unsupervised method that will fill the knowledge gap giving an insight into context-specific signatures of cell types/cell states in cancer Deliver well-documented and user-friendly tool that can be used by the scientific community Decompose a big corpus of bulk omic data into interpretable biological functions, with a particular focus on the immune cell types Use different data types (scRNAseq, microarray, RNAseq, FACS, etc.) to complete, compare and contrast findings of the analysis. Decompose established immune cells populations from metastatic melanoma in order to better understand cell-type heterogeneity In order to face these challenges, I have first focused on testing and creating new methods. This is why methods and results are interlaced in this thesis. Reproducing work of other researchers it is not always easy, and sometimes it is even impossible. Much time was invested in understanding and reusing previous publications, part of this effort was reflected in the introduction, some of my thoughts will be expressed in the discussion. Next important step was improving and testing ideas born in our team. I collaborated to a publication on a topic Chapter 3, and I have authored an extension of this work described in Chapter 4. I have also compared my tool to other similar methods, an overview of the results are in Chapter 5. Once I have found the most appropriate way to apply my method, that I validated with multiple datasets, I have built a tool to share it with the scientific community. The tool is freely available online as an R package. During my work, I have collected many datasets of tumor signatures, tumor metagenes, benchmark datasets some of which are part of my tool. I have also accessed, thanks to the courtesy of our collaborators a collection of pan-cancer bulk transcriptomic datasets that I competed with other publicly available datasets. I build my working environment in which I managed and cleaned the data. Finally, I realized a pan-cancer analysis of over 100 datasets which is the primary outcome of my work. I completed results of this work with published scRNAseq data from tumor samples. This analysis is a source of precious information, I have, so far, explored only part of possible direction focusing on cancer infiltrating T-cells. This results will be found in a manuscript in preparation in Chapter 6. However, more information can still be extracted in the further work. There is also a possibility to provide experimental validation of my finding, and it will be considered in the discussion part. In parallel, I used part of methods to study cell-type heterogeneity in an independent project resulted in a submitted publication (Chapter 8). The remaining time, I have invested in collaborations and contributions to different works within and outside of my team. Published work from those projects will be shortly described in Annexes. "],
["mstd.html", "Chapter 3 Determining the optimal number of independent components for reproducible transcriptomic data analysis", " Chapter 3 Determining the optimal number of independent components for reproducible transcriptomic data analysis Ulykbek Kairov\\(^\\star\\), Laura Cantini\\(^\\star\\), Alessandro Greco, Askhat Molkenov, __Urszula Czerwinska__, Emmanuel Barillot and Andrei Zinovyev Here is text that places article in the context Background Independent Component Analysis (ICA) is a method that models gene expression data as an action of a set of statistically independent hidden factors. The output of ICA depends on a fundamental parameter: the number of components (factors) to compute. The optimal choice of this parameter, related to determining the effective data dimension, remains an open question in the application of blind source separation techniques to transcriptomic data. Results Here we address the question of optimizing the number of statistically independent components in the analysis of transcriptomic data for reproducibility of the components in multiple runs of ICA (within the same or within varying effective dimensions) and in multiple independent datasets. To this end, we introduce ranking of independent components based on their stability in multiple ICA computation runs and define a distinguished number of components (Most Stable Transcriptome Dimension, MSTD) corresponding to the point of the qualitative change of the stability profile. Based on a large body of data, we demonstrate that a sufficient number of dimensions is required for biological interpretability of the ICA decomposition and that the most stable components with ranks below MSTD have more chances to be reproduced in independent studies compared to the less stable ones. At the same time, we show that a transcriptomics dataset can be reduced to a relatively high number of dimensions without losing the interpretability of ICA, even though higher dimensions give rise to components driven by small gene sets. Conclusions We suggest a protocol of ICA application to transcriptomics data with a possibility of prioritizing components with respect to their reproducibility that strengthens the biological interpretation. Computing too few components (much less than MSTD) is not optimal for interpretability of the results. The components ranked within MSTD range have more chances to be reproduced in independent studies. (Kairov et al. 2017) References "],
["lva.html", "Chapter 4 Application of Independent Component Analysis to Tumor Transcriptomes Reveals Specific and Reproducible Immune-Related Signals", " Chapter 4 Application of Independent Component Analysis to Tumor Transcriptomes Reveals Specific and Reproducible Immune-Related Signals Urszula Czerwinska, Laura Cantini, Ulykbek Kairov, Emmanuel Barillot, Andrei Zinovyev Independent Component Analysis (ICA) can be used to model gene expression data as an action of a set of statistically independent hidden factors. The ICA analysis with a downstream component analysis was successfully applied to transcriptomic data previously in order to decompose bulk transcriptomic data into interpretable hidden factors. Some of these factors reflect the presence of an immune infiltrate in the tumor environment. However, no foremost studies focused on reproducibility of the ICA-based immune-related signal in the tumor transcriptome. In this work, we use ICA to detect immune signals in six independent transcriptomic datasets. We observe several strongly reproducible immune-related signals when ICA is applied in sufficiently high-dimensional space (close to one hundred). Interestingly, we can interpret these signals as cell-type specific signals reflecting a presence of T-cells, B-cells and myeloid cells, which are of high interest in the field of oncoimmunology. Further quantification of these signals in tumoral transcriptomes has a therapeutic potential. (Czerwinska et al. 2018) References "],
["comparison-of-reproducibility-between-nmf-and-ica.html", "Chapter 5 Comparison of reproducibility between NMF and ICA 5.1 ? Impact of modification of signatures list on result for signature-based deconvolution methods", " Chapter 5 Comparison of reproducibility between NMF and ICA NMF and ICA are both algorithms often applied to solve blind source deconvolution problem. NMF gained a popularity as a tool of transcriptomic analysis mainly thanks to the publications [publicaition_list]. However, the non-negativity contraint, an attractive concept in the case of non-negative transcriptome counts, may be a reason why the reusults of NMF decomposition are not the best candidate for our deconvolution task. We observed that NMF-based metagenes are less reprouctible between different transcriptomic datasets than ICA-based metagenes. 5.0.1 Comparing metagenes obtained with NMF vs ICA. We compared the reproducibility of NMF and ICA through decomposition of four breast cancer datasets (BRCATCGA, METABRIC, BEK, WAN)[ref]. Those datasets were selected because of their size (number of samples &gt; 50) and because they were available in not centred format necessary for NMF. For NMF the procedure was following: data was transformed into log2 zero rows were removed the algorithm assesing cophentic index was applied to chose optimal number of components datasets were decomposed with matlab NMF implementation from Brunet et al. (???) into (i) number of components suggested by cophenetic coefficient (ii) MSTD dimension (iii) 50 components (approaching overdecomposition) the obtained metagenes were decorellated from the mean using a linear regression model For ICA, the procedure was following: data were transformed into log2 transformed data were mean-centered by gene our implementation of MSTD (most stable transcriptomic dimension) from (Kairov et al. 2017) was used to evaluate most stable dimension datasets were demposed into (i) MSTD dimension and (ii) 50 components (approaching overdecomposition) with matlab implementantion of fastICA with icasso stabilisation We did not decompose ICA into low number of components as we consider it as strong underdecomposition and we suspect signals would not be the most reproducible. We limited the over decomposition higher than 50 with NMF as for our biggest dataset (METABRIC) NMF decomposition into 50 took 30245 minutes (3 weeks). Then separately for NMF and ICA, we correlated all obtained metagens with each other and with known Biton et al. metagenes (obtain from previous ICA decompostion applied pan-cancer). We represented the results in a form of a correlation graph where nodes are metagenes from different datasets and decompostion levels and edge width corresponds peasron correaltion coefficients (Fig 5.1). Figure 5.1: Correlation graph of ICA and NMF multiple decompositions. In the upper part of the figure (A,B) we observe the correlation graph of all metagenes (ICA or NMF-based) disposed using edge-weighted bio layout. In the lower part of the figure (C,D) we applied &gt;0.4 thereshold in order to filter the edges. In the case of ICA (C), remaining nodes form pseudo-cliques, immune-related pseudo-clique is highlighted. In the case of NMF (D), components cluster by dataset. Edges’ width coressponds to Pearson correlation coefficient. Node colors correspond to dataset from which a metagene was obtained (see legend). We hoped to observe a subset of components from different datasets (no matter the decomposition level) correlate with each strongly and much less with other components in order to confirm that the signal is reproducible (can be found in several dataset) and specific. We used the Biton et al. componets here to help with eventual identification of signals (labelling). What we observe from ICA-decomposition that indeed, without applying any threshold some emerging clusters can be remarked and after application of &gt;0.4 threshold on the correlation coeffcient pseudo-cliques emerge. While metagenes from NMF-decpmpostion are more tighlty connected globally and when the threshold is applied, remaing metagenes do not form clear clusters but group by data set. In NMF decomposition if it hard to define different signals as the datasets seem to be all related to each other. We can see from (Fig 5.1D) that the IMMUNE signal is correlated &gt;0.4 with a high number of NMF components that are also linked to some other components. In ICA (Fig 5.1C) components related to the IMMUNE metagens form a pseudo-clique that is related with one link to INTERFERON metagene. This simple analysis illustrates that NMF applied to cancer transcriptomes decomposes them to metagenes that are not highly reproductible between datasets. In practice, it will not always be possible to work with big cohorts and the same processing methods. Using ICA for decomposition gives mor credit that it will be possible to use the obtained metagenes as reference in which new data of similar type could be projected. to do: quantify: with clustering coefficient? Explain why ICA is more reproducible 5.1 ? Impact of modification of signatures list on result for signature-based deconvolution methods Carry on a “sensitivity study”: remove some % of genes from basis matrix or marker gene list evaluate how it changes results References "],
["deconica.html", "Chapter 6 Deconvolution of transcriptomes and methylomes 6.1 From blind deconvolution to cell-type quantification: general overview 6.2 DeconICA R package for ICA-based deconvolution", " Chapter 6 Deconvolution of transcriptomes and methylomes We describe our methods in this chapter. The pre-eliminary pipeline and simple results are described in the manuscript submitted to Springer-Verlag’s Lecture Notes in Computer Science (LNCS) entitled Application of Independent Component Analysis to Tumor Transcriptomes Reveals Specific And Reproducible Immune-related Signals that is placed at the end of this chapter. In the final thesis final pipeline will be split into following structure 6.1 From blind deconvolution to cell-type quantification: general overview Few lines describing our idea Figure? 6.1.1 The ICA-based deconvolution of Transcriptomes remind shortly ICA describe stabilisation procedure icasso explain IC-metagene concept If completed add related section about two other ways of getting metagenes attractor metagenes k-lines 6.1.2 Interpretation of Independent components 6.1.2.1 Correlation based identification of confounding factors 6.1.2.2 Identification of immune cell types with enrichment test / other 6.1.3 Transforming metagenes into signature matrix 6.1.4 Regression-based estimation of cell-type proportions : solving system of equations 6.2 DeconICA R package for ICA-based deconvolution This part of the chapter will be adapted from package vignettes It will contain technical package description user guide examples 6.2.1 Demo The package needs to installed and then imported. #import package library(deconica) Then we can perform our pipeline on sample data available in the package #import sample data data(BRCA) #decompose data fastica.res &lt;- run_fastica ( BRCA, optimal = TRUE, row.center = TRUE, with.names = TRUE, gene.names = NULL, alg.typ = &quot;parallel&quot;, method = &quot;C&quot;, n.comp = 100, isLog = TRUE, R = TRUE ) #correlate obtained metagenes with Biton et al. #metagenes (by default) correlate.res &lt;- correlate_metagenes(fastica.res$S, fastica.res$names) #assign reciprocal components assign.res &lt;- assign_metagenes(correlate.res$r) #identify components that are &gt;0.1 correlated with #immune and are not assigned to any other component identify.immune &lt;- identify_immune_ic(correlate.res$r[, &quot;M8_IMMUNE&quot;], assign.res[, 2]) #test enrichment with fisher test in #Immgen signatures (by default) enrichment.res &lt;- gene.enrichment.test( fastica.res$S, fastica.res$names, names(identify.immune), gmt = ImmgenHUGO, alternative = &quot;greater&quot;, p.adjust.method = &quot;BH&quot;, p.value.threshold = 0.05 ) The present state of the package is described in Fig 6.1. Figure 6.1: State of the deconICA package in January 2018. The flow chart illustrates existing functions in the R package DeconICA. Squares represent functions, red are user-provided inputs, brown are inputs we provide but that can be replaced easily by user and in blu we marked outputs. Next step will be: adding the metagenes selection and transformation into basis matrix for deconvolution identifying confounding factors estimating purity with an existing tool running an equations solver (based on least squares or other type of regression) including basis matrix, confounding factors, purity including regularisation factos adding graphics adding user interface writing a demo (best interactive) "],
["results.html", "Chapter 7 Comparative analysis of cancer immune infiltration", " Chapter 7 Comparative analysis of cancer immune infiltration This chapter will include biological interpretation of Pan-cancer analysis with DeconICA application to Breast cancer compare metagenes of the same cell type in different datasets compare metagenes of the same cell type in the same dataset (happens sometimes) compare A matrix (sample weights) with clinical metadata compare patients with opposite extreme phenotypes (the gene expression) with DEG ou others run enrichment with more specific list of genes ex. Th1/2/17 cells in T cels etc. application pan cancer derivation of meta-metagenes for immune cell types above points are true for pan cancer follow up of Biton paper ? Idea of Vassili from the lab meeting, personally I am not sure if there is no conflict of interest with other members of the team "],
["map.html", "Chapter 8 Heterogeneity of immune cell types", " Chapter 8 Heterogeneity of immune cell types We include here an extract of a ready to submit article of Kondratova et al. (co-first authored by Urszula Czerwinska) - the abstract and figures which are result of work on single cell heterogeneity. Explication how deconvolution methodology can be used for analysis of heterogeneity of immune cells describe the context briefly describe more in details my part - data analysis of single cell data To be defined: add CAFS (that will maybe appear in JBM) add unpublished analysis made for the Nature Immunology Michea et al. paper (to be defined) The single T-cell study (if done) "],
["discussion.html", "Chapter 9 Discussion", " Chapter 9 Discussion "],
["conclusions.html", "Chapter 10 Conclusions and perspectives", " Chapter 10 Conclusions and perspectives Here we will have some interesting and well-written conclusion that will validate the quality of this thesis. Problems: reproducibility of other tools (code accessing, making work, pre-processing) no-spatial dimension heterogeneity - analyse sample by samples (need of big dimension) time dimention validation - no gold statndard our solution - more contex specific but less interpretable? A major part of this thesis has been to reproduce earlier work[7][8], and it has been time consuming to try to reproduce different approaches or scripts. It has been brought up that other scientists have struggled - and many failed - to reproduce another scientists work[48]. The article states that of 1,576 researchers, over 70% have failed to reproduce others work and over 50% have failed to reproduce their own. 52% of the participants in the survey state that is is a “significant crisis”, which indicates that we could call this a “reproducibility crisis”[48]. Such high numbers may suggest in- accurate or poor documentation of the different steps towards achieving the results, or even going as far as suggesting untrustworthy results. The latter is a bold statement, but according to the article, less than 31% believe that struggles to reproduce published results are due to wrong results[48]. Moreover, the immune cells can be situated in different locations, either in the core, in the invasive margin or in the adjacent tertiary lymphoid structures (TLS), ectopic lymphoid formations found in inflamed, infected, or tumoral tissues exhibiting all the characteristics of structures in the lymph nodes (LN) associated with the generation of an adaptive immune response (Dieu-Nosjean et al. 2014): a correlation has been found between high densities of TLS and prolonged patient’s survival in more than 10 different types of cancer (Sautès-Fridman et al. 2016). For instance, CD8+ T cells can be visible in both the invasive margin and the core of the tumor, while the TLS seem to lack these cells. In addition, the mixture of immune cells can vary differently in relation to tumor types. Some components of the immune contexture, more than others, are helpful in terms of good prognosis: this fact is shared by multiple papers, such as Dave et al. 2004, which paved the way in the early years of the XXI century, while in Parker et al. 2008 and Parker et al. 2009 —- . In the case of T cells and cancer, although the total frequencies of tumor-specific T cells are difficult to assess (due to uncertainties about the range of targets, see below), reports of blood-derived tumor-specific T cells suggest that these frequencies are low (much less than 1% of CD8þ T cells, which typically make up 10%–20% of peripheral blood mononuclear cells; refs. 50, 51). Therefore, at least in the case of blood, a large part of the signal measured in bulk T-cell profiles should originate from irrelevant cells. In tumor tissues, the problem is certainly less severe, in that tumor-infiltrating T cells are likely enriched for tumor-specific T cells. However, as discussed above, because the immunologic composition of tumor tissues is complex and heterogeneous, similar efforts dedicated to identifying, quantifying, and profiling relevant (tumor-specific) T cells are still needed. "],
["annexes.html", "Annexes Dc subtypes DreamIdea Challenge Full list of publications CV", " Annexes Dc subtypes DreamIdea Challenge Full list of publications CV "],
["post-scriptum-thesis-writing.html", "Post Scriptum: Thesis writing", " Post Scriptum: Thesis writing This Thesis is written in bookdown. I have chosen this form as it can easily compile to LaTeX, PDF, MS Word, ebook and html. Optimally, the final manuscript will be also published online in a form of an open source gitBook and an ebook including interactive figures and maybe even data demos. Another good reason for using bookdown is its simple syntax of markdown and natural integration of code snippets with .Rmd. It reduces formatting time and give multiple outputs. The template of for this thesis manuscript was adapted from LaTeX template provided by University Paris Descartes. Citations are stocked in Mendeley Desktop and exported to .bib files automatically. "],
["references.html", "References", " References "]
]
